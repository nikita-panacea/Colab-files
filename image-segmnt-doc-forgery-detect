{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11192869,"sourceType":"datasetVersion","datasetId":6987511},{"sourceId":11433806,"sourceType":"datasetVersion","datasetId":7161448}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport ast\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nimport torchvision.transforms as T\nfrom skimage.transform import resize\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:27.736082Z","iopub.execute_input":"2025-04-17T05:31:27.736307Z","iopub.status.idle":"2025-04-17T05:31:34.982406Z","shell.execute_reply.started":"2025-04-17T05:31:27.736289Z","shell.execute_reply":"2025-04-17T05:31:34.981691Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Preprocessing: Convert bounding boxes to segmentation masks\ndef create_mask_from_bboxes(image_shape, annotations, num_classes):\n    mask = Image.new('I', image_shape, 0)  # 'I' for integer mode\n    draw = ImageDraw.Draw(mask)\n    \n#     # Example mapping, adjust according to your 'Entity type'\n#     class_mapping = {'Forgery': 1}\n    \n    if annotations != 0:\n        for ann in annotations[\"regions\"]:\n            bbox = ann['shape_attributes']\n            draw.rectangle([bbox['x'], bbox['y'], bbox['x'] + bbox['width'], bbox['y'] + bbox['height']], fill=1)\n    \n    # Ensure the mask is in the correct format for training\n    mask = np.array(mask)\n    mask = np.clip(mask, 0, num_classes-1)  # Ensure mask values are within [0, num_classes-1]\n    return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:34.983254Z","iopub.execute_input":"2025-04-17T05:31:34.983647Z","iopub.status.idle":"2025-04-17T05:31:34.991538Z","shell.execute_reply.started":"2025-04-17T05:31:34.983625Z","shell.execute_reply":"2025-04-17T05:31:34.990787Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Custom Dataset\nclass SegmentationDataset(Dataset):\n    def __init__(self, image_dir, annotations, transform=None):\n        self.image_dir = image_dir\n        self.annotations = annotations\n        self.transform = transform\n        # Set a common size for all images and masks\n        self.resize = transforms.Resize((256, 256))\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations.iloc[idx]\n        image_path = os.path.join(self.image_dir, ann['image'])\n        image = Image.open(image_path).convert('RGB')\n        mask = create_mask_from_bboxes(image.size, ann[\"forgery annotations\"], num_classes=2)\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            mask = self.transform(mask)\n        \n        mask = np.array(mask, dtype=np.int64)  # Ensure mask is an integer type\n        mask = torch.tensor(mask).squeeze(0)\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:34.993838Z","iopub.execute_input":"2025-04-17T05:31:34.994635Z","iopub.status.idle":"2025-04-17T05:31:35.013516Z","shell.execute_reply.started":"2025-04-17T05:31:34.994606Z","shell.execute_reply":"2025-04-17T05:31:35.012786Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = {}\nfor mode in [\"train\", \"test\"]:\n    df[mode] = pd.read_csv(f\"/kaggle/input/find-it-again-dataset/findit2/{mode}.txt\")\n    df[mode][\"forgery annotations\"] = df[mode][\"forgery annotations\"].map(ast.literal_eval)\n\ndf[\"val\"] = pd.read_csv(f\"/kaggle/input/val-annotation/val_annotations.txt\")\ndf[\"val\"][\"forgery annotations\"] = df[\"val\"][\"forgery annotations\"].map(ast.literal_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:35.014281Z","iopub.execute_input":"2025-04-17T05:31:35.014519Z","iopub.status.idle":"2025-04-17T05:31:35.151482Z","shell.execute_reply.started":"2025-04-17T05:31:35.014499Z","shell.execute_reply":"2025-04-17T05:31:35.150720Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df[\"test\"][\"forged\"].sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:35.152411Z","iopub.execute_input":"2025-04-17T05:31:35.152673Z","iopub.status.idle":"2025-04-17T05:31:35.159209Z","shell.execute_reply.started":"2025-04-17T05:31:35.152650Z","shell.execute_reply":"2025-04-17T05:31:35.158392Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"35"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Transform\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((256, 256)),\n\n])\n\n# Example annotations list (your dataset)\nannotations = df[\"train\"]\n\n# Initialize dataset\ntr_dataset = SegmentationDataset(\"/kaggle/input/find-it-again-dataset/findit2/train\", df[\"train\"], transform=transform)\nva_dataset = SegmentationDataset(\"/kaggle/input/find-it-again-dataset/findit2/val\", df[\"val\"], transform=transform)\nte_dataset = SegmentationDataset(\"/kaggle/input/find-it-again-dataset/findit2/test\", df[\"test\"], transform=transform)\n\ntr_dataloader = DataLoader(tr_dataset, batch_size=4, shuffle=True, drop_last=True)\nva_dataloader = DataLoader(va_dataset, batch_size=4, shuffle=True, drop_last=True)\nte_dataloader = DataLoader(te_dataset, batch_size=4, shuffle=True, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:35.159830Z","iopub.execute_input":"2025-04-17T05:31:35.160066Z","iopub.status.idle":"2025-04-17T05:31:35.175139Z","shell.execute_reply.started":"2025-04-17T05:31:35.160049Z","shell.execute_reply":"2025-04-17T05:31:35.174454Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Model\nmodel = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=False, num_classes=2)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodel.to(device)\n\n\nclass_frequencies = [1.2, 1]  # Example frequencies for three classes\n\n# Calculate weights\nweights = 1.0 / torch.tensor(class_frequencies, dtype=torch.float32)\nweights = weights.to(device)\n\n# Training essentials\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nloss_function = torch.nn.CrossEntropyLoss(weight=weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:35.175898Z","iopub.execute_input":"2025-04-17T05:31:35.176164Z","iopub.status.idle":"2025-04-17T05:31:35.995188Z","shell.execute_reply.started":"2025-04-17T05:31:35.176142Z","shell.execute_reply":"2025-04-17T05:31:35.994386Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 93.7MB/s]\n","output_type":"stream"},{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Training loop\ntr_loss = []\nva_loss = []\ntr_iou = []\nva_iou = []\nnum_epochs = 20  # Example, adjust as needed\n\ndef calculate_iou(preds, masks):\n    intersection = (preds & masks).float().sum((1, 2))\n    union = (preds | masks).float().sum((1, 2))\n    iou = (intersection + 1e-6) / (union + 1e-6)\n    return iou.mean().item()\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, masks in tqdm(tr_dataloader):\n        images, masks = images.to(device), masks.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)['out']\n        loss = loss_function(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    tr_loss.append(running_loss / len(tr_dataloader))\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, masks in va_dataloader:\n            images, masks = images.to(device), masks.to(device)\n            output = model(images)['out']\n            loss = loss_function(output, masks)\n            val_loss += loss.item()\n    va_loss.append(val_loss / len(va_dataloader))\n    print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(tr_dataloader)}, Val Loss: {val_loss / len(va_dataloader)}')\n\ndef compute_iou(pred, target, threshold):\n    pred = (pred > threshold).float()\n    intersection = torch.logical_and(pred, target).float().sum((1, 2))\n    union = torch.logical_or(pred, target).float().sum((1, 2))\n    iou = (intersection / union)\n    return iou.mean().item()\n\n# Test set evaluation\ntest_loss = 0.0\niou_scores_set = []\nthreshold_range = np.arange(0, 1.01, 0.01)\nmodel.eval()\nwith torch.no_grad():\n    for images, masks in tqdm(te_dataloader):\n        images, masks = images.to(device), masks.to(device)\n        output = model(images)['out']\n        loss = loss_function(output, masks)\n        test_loss += loss.item()\n        iou_scores = []\n        \n        output = torch.sigmoid(output)[:, 1, :, :]  # Apply sigmoid to get probabilities and select forged class\n        for threshold in threshold_range:\n            iou = compute_iou(output, masks, threshold)\n            iou_scores.append(iou)\n        iou_scores_set.append(iou_scores)\n\navg_iou = np.mean(iou_scores_set, axis=0)\n\nprint(f'Test Loss: {test_loss / len(te_dataloader)}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:31:35.995887Z","iopub.execute_input":"2025-04-17T05:31:35.996121Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 144/144 [02:54<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Train Loss: 0.28784025615702075, Val Loss: 0.12604237409929434\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:26<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Train Loss: 0.0618898340423281, Val Loss: 0.06137759025053432\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:26<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Train Loss: 0.0350689890070094, Val Loss: 0.05423705776532491\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:25<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Train Loss: 0.026681868695757456, Val Loss: 0.027898234664462507\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:27<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Train Loss: 0.021524354162263788, Val Loss: 0.023537765877942245\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:27<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Train Loss: 0.018680131787227258, Val Loss: 0.030765443390312914\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:26<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Train Loss: 0.017961864023365907, Val Loss: 0.021580849366728216\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:26<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Train Loss: 0.013740272736564899, Val Loss: 0.019949942123882163\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:25<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Train Loss: 0.012049282807210047, Val Loss: 0.02781737053980275\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:26<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train Loss: 0.016609842646478985, Val Loss: 0.039282505303466074\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:25<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Train Loss: 0.011728593680244457, Val Loss: 0.024207305599702522\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 144/144 [02:26<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Train Loss: 0.01094232050753716, Val Loss: 0.02310397404168422\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 111/144 [01:45<00:25,  1.32it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,7))\nepochs = list(range(1, 21))\nplt.plot(epochs, tr_loss, 'r', label='train')\nplt.plot(epochs, va_loss, 'b', label='val')\nplt.axhline(test_loss / len(te_dataloader), color='black', linestyle='--', label='test') \nplt.ylabel(\"cross entropy loss\")\nplt.xlabel(\"epochs\")\nplt.title(\"Segmentation model training progress\")\nfilepath(f'/kaggle/working/trian_val-loss.png')\nplt.savefig(filepath)\nplt.legend()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the IoU curve\nplt.plot(threshold_range, avg_iou, marker='o')\nplt.xlabel('Threshold')\nplt.ylabel('Average IoU')\nplt.title('Average IoU vs. Threshold')\nplt.grid(True)\nfilepath(f'/kaggle/working/ioc_curve.png')\nplt.savefig(filepath)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode = \"test\"\n\ntransform = T.Compose([\n    T.Resize((256, 256)),\n    # Don't convert to tensor yet to facilitate visualization steps\n])\n\nimage_path = f'/kaggle/input/find-it-again-dataset/findit2/test/X51005568866.png'\n## image_path = f'./findit2/torchdataset/test/1/X51005757243.png'\n# image_path = f'./findit2/torchdataset/test/1/X51006557117.png'\n# image_path = f'./findit2/torchdataset/test/1/X51006557195.png'\n\n# Load the original image\noriginal_image = Image.open(image_path).convert('RGB')\noriginal_size = original_image.size  # Width, Height\n\n# Apply the same transformations as during training (excluding ToTensor)\ntransformed_image = transform(original_image)  # Resized image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()  # Ensure the model is in evaluation mode\nwith torch.no_grad():\n    input_tensor = T.functional.to_tensor(transformed_image).unsqueeze(0).to(device)  # Add batch dimension and transfer to GPU\n    output = model(input_tensor)['out']\n    predictions = torch.argmax(output.squeeze(), dim=0).detach().cpu().numpy()  # Convert to numpy array for visualization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_bounding_boxes(image, annotations):\n    draw = ImageDraw.Draw(image)\n    for ann in annotations['regions']:\n        shape_attr = ann['shape_attributes']\n        x, y, w, h = shape_attr['x'], shape_attr['y'], shape_attr['width'], shape_attr['height']\n        draw.rectangle([(x, y), (x+w, y+h)], outline=\"red\", width=4)\n    return image\n\nannotations = df[mode][df[mode][\"image\"]==image_path.split(\"/\")[-1]][\"forgery annotations\"].iloc[0]\n\n# Load the original image without resizing to maintain original dimensions for visualization\noriginal_image = Image.open(image_path).convert('RGB')\ndrawn_image = draw_bounding_boxes(original_image, annotations)  # Assume 'annotations' is defined\n\n# Convert drawn image to array for overlaying\nimport numpy as np\ndrawn_image_array = np.array(drawn_image)\n\n# Create a figure to display the results\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n# Display original image\nax.imshow(drawn_image_array)\n# Overlay heatmap (You may need to adjust the alpha for better visualization)\nheatmap = ax.imshow(predictions_resized, cmap='hot', interpolation='nearest', alpha=0.5)\nplt.colorbar(heatmap)\nfilepath(f'/kaggle/working/heatmap_pred.png')\nplt.savefig(filepath)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_prob = torch.softmax(output, dim=1)[0][1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}