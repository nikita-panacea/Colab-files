{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision pydicom opencv-python matplotlib\n!pip install transformers==4.4.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"0 - Aortic enlargement x\n1 - Atelectasis -->  \"Atelectasis\"\n2 - Calcification x\n3 - Cardiomegaly --> \"Cardiomegaly\"\n4 - Consolidation --> \"Consolidation\"\n5 - ILD x\n6 - Infiltration --> \"Infiltration\"\n7 - Lung Opacity >\n8 - Nodule/Mass >\n9 - Other lesion -\n10 - Pleural effusion \n11 - Pleural thickening >\n12 - Pneumothorax >\n13 - Pulmonary fibrosis >\n\n[\n    \"Atelectasis\" >, \"Consolidation\" >, \"Infiltration\">, \"Pneumothorax\">,\n    \"Edema\", \"Emphysema\"x, \"Fibrosis\">, \"Effusion\">, \"Pneumonia\"x,\n    \"Pleural_Thickening\">, \"Cardiomegaly\">, \"Nodule\", \"Mass\", \"Hernia\",\n    \"Lung Lesion\"-, \"Fracture\", \"Lung Opacity\">, \"Enlarged Cardiomediastinum\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport torch\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport pydicom\nimport cv2  # to help convert DICOM to uint8\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms as T\nfrom torchvision.ops import MultiScaleRoIAlign\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom transformers import AutoConfig, AutoModel\nfrom tqdm import tqdm\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ────────────────────────────────────────────────────────────────────────\n# 1) GLOBAL CONFIGURATION (adjust these as needed)\n# ────────────────────────────────────────────────────────────────────────\n\n# ─── Paths to VinDr data ───────────────────────────────────────────────\nVINDR_ROOT       = \"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/\"            # <— change this\nDICOM_TRAIN_DIR  = os.path.join(VINDR_ROOT, \"train\")\nANNOTATION_CSV   = os.path.join(VINDR_ROOT, \"train.csv\")\n# (optionally, you can have a separate val split; here we’ll just split train→train/val ourselves)\nCHECKPOINT_DIR   = \"./checkpoints\"                  # where to save model ckpts\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# ─── Training hyperparameters ──────────────────────────────────────────\nNUM_EPOCHS        = 12\nBATCH_SIZE_TRAIN  = 4      # reduce if you run out of GPU memory\nBATCH_SIZE_VAL    = 8\nNUM_WORKERS       = 4\n\nLEARNING_RATE_BACKBONE = 5e-6    # when fine‐tuning entire backbone\nLEARNING_RATE_HEAD     = 1e-4    # classifier head & detection head\nWEIGHT_DECAY           = 1e-2\n\n# ─── SigLIP ViT settings ───────────────────────────────────────────────\nSIGLIP_MODEL_NAME = \"StanfordAIMI/XraySigLIP__vit-l-16-siglip-384__webli\"\nDEVICE            = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMAGE_SIZE        = 512\nPATCH_SIZE        = 16   # SigLIP ViT‐L uses 16×16 patches\nNUM_CLASSES       = 18   # VinDr has 18 disease categories (no background)\nCLASS_NAMES       = [\n    \"Atelectasis\", \"Consolidation\", \"Infiltration\", \"Pneumothorax\",\n    \"Edema\", \"Emphysema\", \"Fibrosis\", \"Effusion\", \"Pneumonia\",\n    \"Pleural_Thickening\", \"Cardiomegaly\", \"Nodule\", \"Mass\", \"Hernia\",\n    \"Lung Lesion\", \"Fracture\", \"Lung Opacity\", \"Enlarged Cardiomediastinum\"\n]\n# Build a mapping from disease name → integer label:\nCLASS2IDX = {cls_name: idx+1 for idx, cls_name in enumerate(CLASS_NAMES)}\n# (background class = 0)\n\n# ────────────────────────────────────────────────────────────────────────\n# 2) UTILITY FUNCTIONS: DICOM → PIL, parse VinDr CSV, etc.\n# ────────────────────────────────────────────────────────────────────────\n\ndef dicom_to_pil(dicom_path: str) -> Image.Image:\n    \"\"\"\n    Read a DICOM file, convert to 8‐bit grayscale, then to RGB PIL.\n    This uses a simple windowing approach: scale pixel intensities to [0,255].\n    \"\"\"\n    ds = pydicom.dcmread(dicom_path)\n    arr = ds.pixel_array.astype(np.float32)\n    # normalize to [0, 255]\n    arr -= arr.min()\n    arr /= arr.max() + 1e-6\n    arr = (arr * 255.0).clip(0, 255).astype(np.uint8)\n    # convert to 3‐channel by cv2.cvtColor\n    arr_rgb = cv2.cvtColor(arr, cv2.COLOR_GRAY2RGB)\n    pil = Image.fromarray(arr_rgb)\n    return pil\n\ndef load_vindr_annotations(csv_path: str):\n    \"\"\"\n    Parse the VinDr CSV (image_id, x_min, y_min, width, height, conf, label).\n    Returns a dict:\n      { \"00000001\": [ { \"bbox\": [x1, y1, x2, y2], \"label\": <int> },  … ],  … }\n    Coordinates in pixel‐space (original DICOM size).\n    \"\"\"\n    df = pd.read_csv(csv_path)\n    # We assume columns: image_id, x_min, y_min, width, height, Confidence, class_id\n    # If column names differ, adjust below accordingly.\n    records = {}\n    for _, row in df.iterrows():\n        image_id = str(row[\"image_id\"])\n        x, y, w, h = float(row[\"x_min\"]), float(row[\"y_min\"]), float(row[\"width\"]), float(row[\"height\"])\n        x1, y1 = x, y\n        x2, y2 = x + w, y + h\n        label_name = str(row[\"class_id\"])\n        if label_name not in CLASS2IDX:\n            # skip unknown labels\n            continue\n        lbl = CLASS2IDX[label_name]\n        entry = {\"bbox\": [x1, y1, x2, y2], \"label\": lbl}\n        records.setdefault(image_id, []).append(entry)\n    return records\n\n# ────────────────────────────────────────────────────────────────────────\n# 3) DATASET CLASSES\n# ────────────────────────────────────────────────────────────────────────\n\nclass VinDrCXRDetectionDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for VinDr CXR detection.\n    Each item returns:\n        image_tensor (3×512×512), \n        target { \"boxes\": FloatTensor[K×4], \"labels\": Int64Tensor[K], \"image_id\": …, \"orig_size\": (H_orig, W_orig) }.\n    \"\"\"\n    def __init__(self, image_dir: str, annotations: dict, transform=None):\n        \"\"\"\n        image_dir: path to folder containing DICOM files, named like \"00000001.dcm\"\n        annotations: dict from image_id (no .dcm) → list of { \"bbox\": [x1,y1,x2,y2], \"label\": idx }\n        transform: a torchvision Transform that takes a PIL image → Tensor sized 3×512×512\n        \"\"\"\n        super().__init__()\n        self.image_dir    = image_dir\n        self.annotations  = annotations\n        self.transform    = transform\n        # Build a list of all image IDs that have at least one annotation\n        # (You could also include images with no boxes, if desired.)\n        self.image_ids = sorted(list(self.annotations.keys()))\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        dicom_path = os.path.join(self.image_dir, f\"{image_id}.dcm\")\n        pil_img    = dicom_to_pil(dicom_path)\n        orig_w, orig_h = pil_img.size\n\n        # Apply transform (resize to 512×512 + normalize → Tensor)\n        img_tensor = self.transform(pil_img)  # [3×512×512], float32\n\n        # Prepare target:\n        anns = self.annotations[image_id]\n        boxes = []\n        labels = []\n        for ann in anns:\n            x1, y1, x2, y2 = ann[\"bbox\"]\n            # Scale bounding box from original DICOM size → 512×512\n            # (since transform does a Resize((512,512))). \n            # We assume a uniform resize, so scale factors:\n            sx = 512.0 / float(orig_w)\n            sy = 512.0 / float(orig_h)\n            boxes.append([x1 * sx, y1 * sy, x2 * sx, y2 * sy])\n            labels.append(ann[\"label\"])\n        boxes = torch.tensor(boxes, dtype=torch.float32)      # [K×4]\n        labels = torch.tensor(labels, dtype=torch.int64)      # [K]\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx]),\n            \"orig_size\": torch.tensor([orig_h, orig_w])\n        }\n        return img_tensor, target\n\n\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate_fn to pass to DataLoader for detection:\n    Each batch is a list of tuples (image_tensor, target_dict).\n    We must return:\n      images_list = [image_tensor_i, …], \n      targets_list = [target_dict_i, …]\n    \"\"\"\n    images = [item[0] for item in batch]\n    targets = [item[1] for item in batch]\n    return images, targets\n\n\n# ────────────────────────────────────────────────────────────────────────\n# 4) BUILD SIGLIP BACKBONE FOR Faster R-CNN\n# ────────────────────────────────────────────────────────────────────────\n\nclass SigLIPBackbone(nn.Module):\n    \"\"\"\n    Wrap the pretrained SigLIP ViT so that its patch tokens become a feature map.\n    We remove the [CLS] token, reshape the patch embeddings into (B, hidden, H, W),\n    and return a dict {\"0\": feature_map}. The Faster R-CNN head will pick up from \"0\".\n    \"\"\"\n    def __init__(self, vision_model):\n        super().__init__()\n        self.vision = vision_model\n        # SigLIP ViT patch embedding/encoder expects 'pixel_values' = [B,3,512,512]\n        self.hidden_dim = vision_model.config.hidden_size  # e.g. 1024 or 2560\n        # For image_size=512, patch_size=16 ⇒ feature map is (512/16)×(512/16) = 32×32\n        self.feature_size = IMAGE_SIZE // PATCH_SIZE       # =32\n\n    def forward(self, x):\n        \"\"\"\n        x: Tensor[B, 3, 512, 512], float32, normalized as SigLIP expects.\n        returns: dict of { \"0\": Tensor[B, hidden_dim, feature_size, feature_size ] }\n        \"\"\"\n        out = self.vision(pixel_values=x, return_dict=True)\n        last_hidden = out.last_hidden_state  # [B, 1 + num_patches, hidden_dim]\n        # Discard [CLS] token at index 0, keep only patch tokens:\n        patch_tokens = last_hidden[:, 1:, :]  # [B, num_patches, hidden_dim]\n        B, N, D = patch_tokens.shape\n        H = W = self.feature_size  # assume N = H*W\n        # reshape into [B, hidden_dim, H, W]\n        feats = patch_tokens.permute(0, 2, 1).reshape(B, D, H, W)\n        return {\"0\": feats}\n\n\ndef make_fasterrcnn_model(num_classes):\n    \"\"\"\n    Create a Faster R-CNN model whose backbone is SigLIPBackbone.\n      - rpn_anchor_generator: choose sizes/aspect_ratios that roughly match typical nodule sizes.\n      - box_roi_pool: ROI align over the feature map \"0\" to 7×7.\n      - box_head, box_predictor: leave default.\n    \"\"\"\n    # ① Load pretrained SigLIP ViT\n    vision_config = AutoConfig.from_pretrained(SIGLIP_MODEL_NAME, trust_remote_code=True)\n    vision_full   = AutoModel.from_pretrained(SIGLIP_MODEL_NAME, config=vision_config, trust_remote_code=True)\n    vision_model  = vision_full.vision_model.to(DEVICE)\n    del vision_full\n    torch.cuda.empty_cache()\n\n    # ② Wrap into backbone\n    backbone = SigLIPBackbone(vision_model)\n    backbone.out_channels = backbone.hidden_dim  # say 1024 or 2560\n\n    # ③ Define an RPN anchor generator with custom sizes/aspect ratios\n    anchor_sizes = ((32,), (64,), (128,), (256,), (512,))  \n    # 32→small nodules; 512→large opacities\n    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n    rpn_anchor_generator = AnchorGenerator(\n        sizes=anchor_sizes,\n        aspect_ratios=aspect_ratios\n    )\n\n    # ④ ROI Pooling: use only the single feature map \"0\"\n    roi_pooler = MultiScaleRoIAlign(\n        featmap_names=[\"0\"], \n        output_size=7, \n        sampling_ratio=2\n    )\n\n    # ⑤ Build Faster R‐CNN\n    model = FasterRCNN(\n        backbone=backbone,\n        num_classes=num_classes + 1,  # +1 for background\n        rpn_anchor_generator=rpn_anchor_generator,\n        box_roi_pool=roi_pooler\n    ).to(DEVICE)\n\n    return model\n\n\n# ────────────────────────────────────────────────────────────────────────\n# 5) TRAIN / VALIDATION LOOPS\n# ────────────────────────────────────────────────────────────────────────\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50):\n    \"\"\"\n    Standard single‐epoch training loop for Faster RCNN.\n    \"\"\"\n    model.train()\n    total_loss = 0.0\n    it = 0\n    pbar = tqdm(data_loader, desc=f\"[Epoch {epoch}][Train]\", leave=False)\n    for images, targets in pbar:\n        images = [img.to(device) for img in images]\n        # Ensure each target[\"boxes\"], target[\"labels\"] on GPU:\n        for t in targets:\n            t[\"boxes\"]  = t[\"boxes\"].to(device)\n            t[\"labels\"] = t[\"labels\"].to(device)\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        batch_loss = losses.item()\n        total_loss += batch_loss\n        it += 1\n        if it % print_freq == 0:\n            pbar.set_postfix(loss=batch_loss)\n    mean_loss = total_loss / it\n    return mean_loss\n\n\n@torch.no_grad()\ndef validate_one_epoch(model, data_loader, device, epoch):\n    \"\"\"\n    Simple validation loop: we report the same total loss as training.\n    (A “proper” detection metric like mAP would require additional code.)\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    it = 0\n    pbar = tqdm(data_loader, desc=f\"[Epoch {epoch}][Val]\", leave=False)\n    for images, targets in pbar:\n        images = [img.to(device) for img in images]\n        for t in targets:\n            t[\"boxes\"]  = t[\"boxes\"].to(device)\n            t[\"labels\"] = t[\"labels\"].to(device)\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        total_loss += losses.item()\n        it += 1\n    mean_loss = total_loss / max(it, 1)\n    return mean_loss\n\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)\n\n\n# ────────────────────────────────────────────────────────────────────────\n# 6) MAIN TRAINING SCRIPT\n# ────────────────────────────────────────────────────────────────────────\n\ndef main():\n    # 6.1) Load VinDr annotations and split into train/val \n    all_annotations = load_vindr_annotations(ANNOTATION_CSV)\n\n    # Do an 80/20 split of image IDs for train vs. val:\n    all_ids = sorted(list(all_annotations.keys()))\n    random.shuffle(all_ids)\n    split_idx = int(0.8 * len(all_ids))\n    train_ids, val_ids = all_ids[:split_idx], all_ids[split_idx:]\n\n    train_anns = {img_id: all_annotations[img_id] for img_id in train_ids}\n    val_anns   = {img_id: all_annotations[img_id] for img_id in val_ids}\n\n    # 6.2) Build Dataset & DataLoader \n    transform = T.Compose([\n        T.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=T.InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(\n            mean=[0.48145466, 0.4578275, 0.40821073],\n            std =[0.26862954, 0.26130258, 0.27577711]\n        ),\n    ])\n\n    train_dataset = VinDrCXRDetectionDataset(DICOM_TRAIN_DIR, train_anns, transform)\n    val_dataset   = VinDrCXRDetectionDataset(DICOM_TRAIN_DIR, val_anns,   transform)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE_TRAIN,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE_VAL,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n\n    print(f\"  → Training on {len(train_dataset)} images, validating on {len(val_dataset)} images.\")\n    print(f\"  → {len(train_loader)} train batches, {len(val_loader)} val batches.\")\n\n    # 6.3) Build model\n    model = make_fasterrcnn_model(NUM_CLASSES)\n    model.to(DEVICE)\n\n    # 6.4) Create optimizer & LR scheduler\n    params = [\n        {\"params\": [p for p in model.backbone.parameters() if p.requires_grad], \n         \"lr\": LEARNING_RATE_BACKBONE, \"weight_decay\": WEIGHT_DECAY},\n        {\"params\": [p for p in model.rpn.parameters() if p.requires_grad]   , \n         \"lr\": LEARNING_RATE_HEAD,     \"weight_decay\": WEIGHT_DECAY},\n        {\"params\": [p for p in model.roi_heads.parameters() if p.requires_grad], \n         \"lr\": LEARNING_RATE_HEAD,     \"weight_decay\": WEIGHT_DECAY},\n    ]\n    optimizer = torch.optim.AdamW(params)\n    total_steps = len(train_loader) * NUM_EPOCHS\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n\n    best_val_loss = float(\"inf\")\n    train_losses, val_losses = [], []\n\n    for epoch in range(1, NUM_EPOCHS + 1):\n        start_time = time.time()\n        train_loss = train_one_epoch(model, optimizer, train_loader, DEVICE, epoch, print_freq=50)\n        val_loss   = validate_one_epoch(model, val_loader, DEVICE, epoch)\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n        print(f\"[Epoch {epoch}/{NUM_EPOCHS}] \" \n              f\"Train Loss={train_loss:.4f}  Val Loss={val_loss:.4f}  \"\n              f\"Time={time.time() - start_time:.1f}s\")\n\n        # Step the scheduler once per epoch (we stepped inside train loop already, but \n        # if you prefer stepping per iteration, keep what's above; skipping now)\n        # scheduler.step()\n\n        # Save checkpoint every epoch\n        ckpt = {\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"scheduler_state\": scheduler.state_dict(),\n            \"train_losses\": train_losses,\n            \"val_losses\": val_losses,\n        }\n        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"fasterrcnn_epoch{epoch}.pt\")\n        save_checkpoint(ckpt, ckpt_path)\n        print(f\"  → Saved checkpoint: {ckpt_path}\")\n\n        # Save “best” if val_loss improved\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_path = os.path.join(CHECKPOINT_DIR, \"fasterrcnn_best.pt\")\n            save_checkpoint(ckpt, best_path)\n            print(f\"  → Saved new BEST checkpoint: {best_path}\")\n\n    # 6.5) Plot Training & Validation Loss Curves\n    epochs = list(range(1, NUM_EPOCHS + 1))\n    plt.figure(figsize=(8,4))\n    plt.plot(epochs, train_losses, label=\"Train Loss\", marker=\"o\")\n    plt.plot(epochs, val_losses,   label=\"Val   Loss\", marker=\"s\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Faster R-CNN Training & Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(os.path.join(CHECKPOINT_DIR, \"loss_curve.png\"))\n    plt.close()\n\n    print(\"Training complete. Loss curve saved.\")\n\n\n# ────────────────────────────────────────────────────────────────────────\n# 7) INFERENCE ON NEW IMAGES (after training)\n# ────────────────────────────────────────────────────────────────────────\n\ndef run_inference_on_folder(model_ckpt_path: str, dicom_folder: str, output_folder: str, score_thresh: float = 0.5):\n    \"\"\"\n    Load the trained Faster R-CNN from `model_ckpt_path`, run inference on all DICOMs \n    in `dicom_folder`, and save images with drawn bounding boxes to `output_folder`.\n    Only boxes with score ≥ score_thresh are drawn.\n    \"\"\"\n    os.makedirs(output_folder, exist_ok=True)\n\n    # 7.1) Reload backbone + head\n    model = make_fasterrcnn_model(NUM_CLASSES)\n    state = torch.load(model_ckpt_path, map_location=\"cpu\")\n    model.load_state_dict(state[\"model_state\"])\n    model.to(DEVICE).eval()\n\n    # 7.2) Loop over each DICOM in folder\n    for fname in os.listdir(dicom_folder):\n        if not fname.lower().endswith(\".dcm\"):\n            continue\n        image_id = os.path.splitext(fname)[0]\n        dicom_path = os.path.join(dicom_folder, fname)\n        pil = dicom_to_pil(dicom_path)\n        orig_w, orig_h = pil.size\n\n        # Preprocess\n        img_tensor = T.Compose([\n            T.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=T.InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(\n                mean=[0.48145466, 0.4578275, 0.40821073],\n                std =[0.26862954, 0.26130258, 0.27577711]\n            )\n        ])(pil).unsqueeze(0).to(DEVICE)  # [1,3,512,512]\n\n        with torch.no_grad():\n            outputs = model(img_tensor)[0]\n            # outputs: dict with \"boxes\" [M×4], \"labels\"[M], \"scores\"[M]\n\n        boxes  = outputs[\"boxes\"].cpu().numpy()\n        labels = outputs[\"labels\"].cpu().numpy()\n        scores = outputs[\"scores\"].cpu().numpy()\n\n        # Filter by score threshold\n        keep = scores >= score_thresh\n        boxes  = boxes[keep]\n        labels = labels[keep]\n        scores = scores[keep]\n\n        # Draw on original PIL\n        draw_img = pil.copy()\n        draw = ImageDraw.Draw(draw_img)\n        # Boxes are in 512×512 coordinate space; scale back to orig:\n        for (x1, y1, x2, y2), lbl, scr in zip(boxes, labels, scores):\n            cls_name = CLASS_NAMES[lbl-1]  # because model outputs [1..NUM_CLASSES]\n            # scale to orig\n            sx = orig_w / IMAGE_SIZE\n            sy = orig_h / IMAGE_SIZE\n            rx1 = x1 * sx\n            ry1 = y1 * sy\n            rx2 = x2 * sx\n            ry2 = y2 * sy\n            draw.rectangle([rx1, ry1, rx2, ry2], outline=\"red\", width=4)\n            draw.text((rx1, ry1 - 10), f\"{cls_name}:{scr:.2f}\", fill=\"white\")\n\n        out_path = os.path.join(output_folder, f\"{image_id}_pred.png\")\n        draw_img.save(out_path)\n        print(f\"  → Saved: {out_path}\")\n\n    print(\"Inference complete.\")\n\n\n# ────────────────────────────────────────────────────────────────────────\n# 8) ENTRY POINT\n# ────────────────────────────────────────────────────────────────────────\n\nif __name__ == \"__main__\":\n    main()\n    # Optionally, after training you can call inference:\n    # run_inference_on_folder(\n    #     model_ckpt_path=os.path.join(CHECKPOINT_DIR, \"fasterrcnn_best.pt\"),\n    #     dicom_folder=\"/path/to/your/test_dicom_folder\",\n    #     output_folder=\"./inference_results\",\n    #     score_thresh=0.5\n    # )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}