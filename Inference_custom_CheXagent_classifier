{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11807802,"sourceType":"datasetVersion","datasetId":7415702},{"sourceId":11808692,"sourceType":"datasetVersion","datasetId":7416346},{"sourceId":11865985,"sourceType":"datasetVersion","datasetId":7456449},{"sourceId":11866001,"sourceType":"datasetVersion","datasetId":7456461},{"sourceId":416775,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":340017,"modelId":361137},{"sourceId":416777,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":340019,"modelId":361139}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.40.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T10:14:56.267733Z","iopub.execute_input":"2025-05-29T10:14:56.267926Z","iopub.status.idle":"2025-05-29T10:15:09.300274Z","shell.execute_reply.started":"2025-05-29T10:14:56.267906Z","shell.execute_reply":"2025-05-29T10:15:09.299561Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.40.0\n  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.40.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.1\n    Uninstalling tokenizers-0.21.1:\n      Successfully uninstalled tokenizers-0.21.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, warnings, time\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import logging\nlogging.set_verbosity_error()\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# ─── 1) PARAMETERS & PATHS ─────────────────────────────────────────────\nTEST_IMAGE_DIR  = \"/kaggle/input/chest-x-ray-images/test_data\"\nGT_CSV          = \"/kaggle/input/chest-x-ray-ground-truth-labels/test_df/test_df.csv\"\nCKPT_DIR        = \"/kaggle/input/\"\nVISION_CKPT     = os.path.join(CKPT_DIR, \"vision_encoder/transformers/default/1/vision_encoder_epoch_12.pt\")\nHEAD_CKPT       = os.path.join(CKPT_DIR, \"classifier_head/transformers/default/1/head_epoch_12.pt\")\nDEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ─── 2) READ GT & DISEASE LIST ─────────────────────────────────────────\ngt_df = pd.read_csv(GT_CSV)\ngt_df[\"ImageName\"] = gt_df[\"ImageName\"].apply(os.path.basename)\nDISEASES = [c for c in gt_df.columns if c != \"ImageName\"]\n\n# ─── 3) MODEL DEFINITION & LOAD ────────────────────────────────────────\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nclass CXRMultiLabel(nn.Module):\n    def __init__(self, vision, num_labels):\n        super().__init__()\n        self.vision = vision\n        in_dim = vision.config.hidden_size\n        self.head = nn.Sequential(\n            nn.Linear(in_dim, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_labels)\n        )\n    def forward(self, pv):\n        out    = self.vision(pixel_values=pv, return_dict=True)\n        cls    = out.last_hidden_state[:, 0]\n        return self.head(cls)\n\n# load vision encoder\nconfig         = AutoConfig.from_pretrained(\"StanfordAIMI/CheXagent-2-3b\", trust_remote_code=True)\nfull           = AutoModelForCausalLM.from_pretrained(\n                    \"StanfordAIMI/CheXagent-2-3b\",\n                    config=config,\n                    torch_dtype=torch.float32,\n                    trust_remote_code=True,\n                    device_map=\"cpu\",\n                    low_cpu_mem_usage=True,\n                ).eval()\nvision_encoder = full.model.visual.model\ndel full; torch.cuda.empty_cache()\nvision_encoder.load_state_dict(torch.load(VISION_CKPT, map_location=\"cpu\"))\nvision_encoder.to(DEVICE)\n\n# build full model\nmodel = CXRMultiLabel(vision_encoder, num_labels=len(DISEASES))\nhead_ckpt = torch.load(HEAD_CKPT, map_location=\"cpu\")\nmodel.head.load_state_dict(head_ckpt[\"head_state\"])\nmodel.to(DEVICE).eval()\n\n# ─── 4) DATASET & DATALOADER ──────────────────────────────────────────\npreprocess = T.Compose([\n    T.Resize((512,512)),\n    T.ToTensor(),\n    T.Normalize([0.48145466,0.4578275,0.40821073],\n                [0.26862954,0.26130258,0.27577711]),\n])\nclass TestDataset(Dataset):\n    def __init__(self, df, img_dir, tf):\n        self.df      = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.tf      = tf\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        name = self.df.loc[idx, \"ImageName\"]\n        img  = Image.open(f\"{self.img_dir}/{name}\").convert(\"RGB\")\n        return name, self.tf(img)\n\ntest_ds     = TestDataset(gt_df[[\"ImageName\"]], TEST_IMAGE_DIR, preprocess)\ntest_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n# ─── 5) INFERENCE w/ PROGRESS & ETA ───────────────────────────────────\nrecords = []\nsigmoid = nn.Sigmoid()\ntotal_images = len(test_ds)\nstart_time   = time.time()\n\nwith torch.no_grad():\n    # we use tqdm over the DataLoader\n    pbar = tqdm(enumerate(test_loader, 1),\n                total=len(test_loader),\n                desc=\"Inferring\",\n                unit=\"batch\")\n    for batch_idx, (img_names, pvs) in pbar:\n        pvs = pvs.to(DEVICE)\n        logits = model(pvs)\n        probs  = sigmoid(logits).cpu().numpy()\n        preds  = (probs >= 0.5).astype(int)\n\n        # accumulate\n        for i, name in enumerate(img_names):\n            row = {\"ImageName\": name}\n            for j, d in enumerate(DISEASES):\n                row[d] = int(preds[i, j])\n            records.append(row)\n\n        # update ETA display\n        images_done = batch_idx * test_loader.batch_size\n        elapsed     = time.time() - start_time\n        eta_s       = (elapsed / images_done) * (total_images - images_done) \\\n                      if images_done else 0\n        pbar.set_postfix({\n            \"images\": f\"{min(images_done,total_images)}/{total_images}\",\n            \"eta_s\": f\"{eta_s:.1f}\"\n        })\n\n# ─── 6) SAVE PREDICTIONS ──────────────────────────────────────────────\npred_df = pd.DataFrame(records, columns=[\"ImageName\"] + DISEASES)\npred_df.to_csv(\"cxr_multilabel_inference.csv\", index=False)\nprint(\"Done. Wrote predictions to cxr_multilabel_inference.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T10:15:09.301269Z","iopub.execute_input":"2025-05-29T10:15:09.301554Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d29c788780e4a6bbbc241abe85e7cd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_chexagent.py:   0%|          | 0.00/9.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eee21586d0e44d26a0b1825fef2140e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_chexagent.py:   0%|          | 0.00/53.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ac506844334766aa93565cba72a38e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_visual.py:   0%|          | 0.00/8.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de729622a9ed49b083f9789da66091b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_chexagent.py:   0%|          | 0.00/26.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75898b026f34ed7bf3fab7bd354dd84"}},"metadata":{}},{"name":"stderr","text":"2025-05-29 10:15:26.746470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748513726.956431      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748513727.013434      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/75.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802892d13f0e408da0b122d5f292acc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb7a70831514729bfb9db556f25c73c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c4496571e14236b909e32bee14eea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae1f4764bf84b1c9bf0043c6c6b1a13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/2.60G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95be077dbc2440948aa891c8a8fdb9e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b6ebfca05c48939a670d59a84cacb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9833e1d5369f42abbcce1d459aeb6e87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddcee500e2ee432fb49e05a05a365283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f7f5b099ab4f3f960723966e96d701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/769 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59080349dc124bca83f17ba8d96cb71f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22cee98b6dd34baf9ef8e05861aa13d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb0aa9865014d7bb2fef0166018424f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ccac06572be4587928104d1c121ad94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639786ed0791449b8c7affaa460fb2cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ee56c7b1dd4709826003ab7b3681b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a613ff476b647aab03c0240af670a4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3e5994c564452999798fca6abbc1ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faa00eb58e5642ad9642c37bf958975b"}},"metadata":{}},{"name":"stderr","text":"Inferring:  47%|████▋     | 229/488 [08:00<09:00,  2.09s/batch, images=3664/7793, eta_s=541.7]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}