{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfLT2i0MLyD"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/rapids-pip-colab-template.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Install RAPIDS into Colab\"/>\n",
        "</a>\n",
        "\n",
        "# RAPIDS cuDF is now already on your Colab instance!\n",
        "RAPIDS cuDF is preinstalled on Google Colab and instantly accelerates Pandas with zero code changes. [You can quickly get started with our tutorial notebook](https://nvda.ws/rapids-cudf). This notebook template is for users who want to utilize the full suite of the RAPIDS libraries for their workflows on Colab.  \n",
        "\n",
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "You can check the output of `!nvidia-smi` to check which GPU you have.  Please uncomment the cell below if you'd like to do that.  Currently, RAPIDS runs on all available Colab GPU instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67T0090Jk2KL",
        "outputId": "1228c4a1-ee91-4e4e-eae8-0d62c3c800d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 15 05:52:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_v33LnDVNo3"
      },
      "source": [
        "#Setup:\n",
        "This set up script:\n",
        "\n",
        "1. Checks to make sure that the GPU is RAPIDS compatible\n",
        "1. Pip Installs the RAPIDS' libraries, which are:\n",
        "  1. cuDF\n",
        "  1. cuML\n",
        "  1. cuGraph\n",
        "  1. cuSpatial\n",
        "  1. cuxFilter\n",
        "  1. cuCIM\n",
        "  1. xgboost\n",
        "\n",
        "# Controlling Which RAPIDS Version is Installed\n",
        "This line in the cell below, `!python rapidsai-csp-utils/colab/pip-install.py`, kicks off the RAPIDS installation script.  You can control the RAPIDS version installed by adding either `latest`, `nightlies` or the default/blank option.  Example:\n",
        "\n",
        "`!python rapidsai-csp-utils/colab/pip-install.py <option>`\n",
        "\n",
        "You can now tell the script to install:\n",
        "1. **RAPIDS + Colab Default Version**, by leaving the install script option blank (or giving an invalid option), adds the rest of the RAPIDS libraries to the RAPIDS cuDF library preinstalled on Colab.  **This is the default and recommended version.**  Example: `!python rapidsai-csp-utils/colab/pip-install.py`\n",
        "1. **Latest known working RAPIDS stable version**, by using the option `latest` upgrades all RAPIDS labraries to the latest working RAPIDS stable version.  Usually early access for future RAPIDS+Colab functionality - some functionality may not work, but can be same as the default version. Example: `!python rapidsai-csp-utils/colab/pip-install.py latest`\n",
        "1. **the current nightlies version**, by using the option, `nightlies`, installs current RAPIDS nightlies version.  For RAPIDS Developer use - **not recommended/untested**.  Example: `!python rapidsai-csp-utils/colab/pip-install.py nightlies`\n",
        "\n",
        "\n",
        "**This will complete in about 5-6 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0C8IV5TQnjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20497d49-3a8b-4f38-f603-3d39df823369"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 587, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 587 (delta 122), reused 85 (delta 82), pack-reused 434 (from 3)\u001b[K\n",
            "Receiving objects: 100% (587/587), 193.00 KiB | 4.49 MiB/s, done.\n",
            "Resolving deltas: 100% (296/296), done.\n",
            "Installing RAPIDS remaining 25.02 libraries\n",
            "error: a value is required for '--prerelease <PRERELEASE>' but none was supplied\n",
            "  [possible values: disallow, allow, if-necessary, explicit, if-necessary-or-explicit]\n",
            "\n",
            "For more information, try '--help'.\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "\n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "\n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZJMJ6BulmMn"
      },
      "source": [
        "# RAPIDS is now installed on Colab.  \n",
        "You can copy your code into the cells below or use the below to validate your RAPIDS installation and version.  \n",
        "# Enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nLrk46BllED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3c768fb-4214-40e0-91a1-bc13d534c249"
      },
      "source": [
        "import cudf\n",
        "cudf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'25.02.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "cuml.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xgAFgI15ddf6",
        "outputId": "9e523afa-e0ef-4129-9238-d1be93f68cf5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'25.02.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P7apPipfHRr5",
        "outputId": "0a9934e8-2595-4773-f58e-a2f3e5c2b37a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Import GPU-enabled GridSearchCV from cuML\n",
        "from cuml.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold  # still using scikit-learn CV splitter\n",
        "\n",
        "# For GPU-based scoring you can usually use the same metric.\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, ConfusionMatrixDisplay,\n",
        "                             roc_curve)\n",
        "\n",
        "# XGBoost import (set tree_method to GPU)\n",
        "import xgboost as xgb\n",
        "\n",
        "# IMPORTANT: For GPU-accelerated models, import cuML estimators where possible.\n",
        "from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
        "from cuml.svm import SVC as cuSVC\n",
        "from cuml.ensemble import RandomForestClassifier as cuRFClassifier\n",
        "# (cuML does not yet have a DecisionTreeClassifier, so you might keep the scikit-learn version for that or omit.)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Ignore some warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "\n"
      ],
      "metadata": {
        "id": "r_SNYo4QDBQL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MERGED_DATA_DIR = Path(\"/content/drive/MyDrive/merged_datasets_csv\") # Directory with train/val/test_merged_data.csv\n",
        "OUTPUT_RESULTS_DIR = Path(\"/content/drive/MyDrive/classification_results_v2\") # V2 for new results\n",
        "OUTPUT_MODELS_DIR = OUTPUT_RESULTS_DIR / \"models\"\n",
        "OUTPUT_PLOTS_DIR = OUTPUT_RESULTS_DIR / \"plots\"\n",
        "\n",
        "# Define file names\n",
        "TRAIN_FILE = MERGED_DATA_DIR / \"train_dataset.csv\"\n",
        "VAL_FILE = MERGED_DATA_DIR / \"val_dataset.csv\"\n",
        "TEST_FILE = MERGED_DATA_DIR / \"test_dataset.csv\"\n",
        "\n",
        "# Target variable\n",
        "TARGET_COLUMN = 'forged'\n",
        "\n",
        "# Original + Annotation Features (Base)\n",
        "BASE_FEATURES = [\n",
        "    'prod_price', 'prod_qty', 'prod_amt', 'total', 'amt_paid',\n",
        "    'change', 'tax', 'discount',\n",
        "    'digital annotation', 'handwritten annotation'\n",
        "]\n",
        "\n",
        "# Models to train\n",
        "MODELS_TO_TRAIN = [\n",
        "    'LogisticRegression',\n",
        "    'SVC',\n",
        "    'RandomForestClassifier',\n",
        "    'DecisionTreeClassifier',\n",
        "    'XGBClassifier'\n",
        "]\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_PLOTS_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "WJ61c-DIDBDq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Engineering Function ---\n",
        "\n",
        "def feature_engineer(df):\n",
        "    \"\"\"Creates new features from existing columns.\"\"\"\n",
        "    print(\"  Starting Feature Engineering...\")\n",
        "    df_eng = df.copy()\n",
        "\n",
        "    # --- 1. Datetime Feature Extraction ---\n",
        "    # Ensure 'datetime' column exists and is datetime type\n",
        "    if 'datetime' in df_eng.columns and pd.api.types.is_datetime64_any_dtype(df_eng['datetime']):\n",
        "        print(\"    Extracting datetime features...\")\n",
        "        dt_col = df_eng['datetime']\n",
        "        df_eng['hour'] = dt_col.dt.hour\n",
        "        df_eng['minute'] = dt_col.dt.minute\n",
        "        df_eng['day_of_week'] = dt_col.dt.dayofweek # Monday=0, Sunday=6\n",
        "        df_eng['day_of_year'] = dt_col.dt.dayofyear\n",
        "        df_eng['month'] = dt_col.dt.month\n",
        "        df_eng['year'] = dt_col.dt.year\n",
        "        df_eng['is_weekend'] = (df_eng['day_of_week'] >= 5).astype(int) # Sat/Sun\n",
        "\n",
        "        # Cyclical Encoding (preserves closeness, e.g., Dec is close to Jan)\n",
        "        print(\"    Applying cyclical encoding...\")\n",
        "        df_eng['hour_sin'] = np.sin(2 * np.pi * df_eng['hour'] / 24)\n",
        "        df_eng['hour_cos'] = np.cos(2 * np.pi * df_eng['hour'] / 24)\n",
        "        df_eng['day_of_week_sin'] = np.sin(2 * np.pi * df_eng['day_of_week'] / 7)\n",
        "        df_eng['day_of_week_cos'] = np.cos(2 * np.pi * df_eng['day_of_week'] / 7)\n",
        "        df_eng['month_sin'] = np.sin(2 * np.pi * df_eng['month'] / 12)\n",
        "        df_eng['month_cos'] = np.cos(2 * np.pi * df_eng['month'] / 12)\n",
        "\n",
        "        # Drop original simple time features if cyclical are used and preferred\n",
        "        # df_eng = df_eng.drop(columns=['hour', 'day_of_week', 'month'])\n",
        "\n",
        "    else:\n",
        "        print(\"    'datetime' column not found or not datetime type. Skipping datetime features.\")\n",
        "\n",
        "\n",
        "    # --- 2. Financial Calculation Features ---\n",
        "    print(\"    Calculating financial features...\")\n",
        "    # Ensure required columns are numeric first, coercing errors\n",
        "    num_cols = ['prod_price', 'prod_qty', 'prod_amt', 'total', 'amt_paid', 'change', 'tax', 'discount']\n",
        "    for col in num_cols:\n",
        "        if col in df_eng.columns:\n",
        "            df_eng[col] = pd.to_numeric(df_eng[col], errors='coerce')\n",
        "        else:\n",
        "            print(f\"      Warning: Column '{col}' needed for financial calcs not found.\")\n",
        "            # Create dummy column with NaN if needed for subsequent steps to not fail immediately\n",
        "            if col not in df_eng.columns: df_eng[col] = np.nan\n",
        "\n",
        "    # Row-wise product check: prod_amt vs (price * qty)\n",
        "    # Handle 0 quantity carefully to avoid division by zero implicitly later if needed\n",
        "    # Handle potential NaNs in calculation\n",
        "    df_eng['prod_calc_diff'] = df_eng['prod_amt'] - (df_eng['prod_price'] * df_eng['prod_qty'])\n",
        "\n",
        "    # Row-wise payment check: total vs (amt_paid - change)\n",
        "    df_eng['payment_check'] = df_eng['amt_paid'] - df_eng['change']\n",
        "    df_eng['payment_vs_total_diff'] = df_eng['total'] - df_eng['payment_check']\n",
        "\n",
        "    # Group-wise calculations (per invoice/file)\n",
        "    if 'file_name' in df_eng.columns:\n",
        "        print(\"    Calculating group-wise features (per file_name)...\")\n",
        "        df_eng['sum_prod_amt_per_invoice'] = df_eng.groupby('file_name')['prod_amt'].transform('sum')\n",
        "        df_eng['product_count_per_invoice'] = df_eng.groupby('file_name')['prod_name'].transform('count') # Assumes prod_name exists\n",
        "\n",
        "        # Difference between declared total and sum of product amounts\n",
        "        df_eng['total_vs_sum_prod_diff'] = df_eng['total'] - df_eng['sum_prod_amt_per_invoice']\n",
        "    else:\n",
        "        print(\"      Warning: 'file_name' column not found. Skipping group-wise features.\")\n",
        "        df_eng['sum_prod_amt_per_invoice'] = np.nan\n",
        "        df_eng['product_count_per_invoice'] = np.nan\n",
        "        df_eng['total_vs_sum_prod_diff'] = np.nan\n",
        "\n",
        "    # Ratios (handle division by zero -> replace inf with 0 or NaN then impute)\n",
        "    print(\"    Calculating financial ratios...\")\n",
        "    # Use a small epsilon to avoid division by zero exactly\n",
        "    epsilon = 1e-6\n",
        "    df_eng['tax_ratio'] = df_eng['tax'] / (df_eng['total'] + epsilon)\n",
        "    df_eng['discount_ratio'] = df_eng['discount'] / (df_eng['total'] + epsilon)\n",
        "    df_eng['change_ratio'] = df_eng['change'] / (df_eng['amt_paid'] + epsilon)\n",
        "\n",
        "    # Replace potential infinities resulting from division by ~zero\n",
        "    df_eng.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "\n",
        "    # --- 3. Indicator Flags ---\n",
        "    print(\"    Creating indicator flags...\")\n",
        "    df_eng['is_discount_applied'] = (df_eng['discount'].fillna(0) > 0).astype(int)\n",
        "    df_eng['is_tax_applied'] = (df_eng['tax'].fillna(0) > 0).astype(int)\n",
        "    df_eng['is_change_given'] = (df_eng['change'].fillna(0) > 0).astype(int)\n",
        "    df_eng['prod_price_is_zero'] = (df_eng['prod_price'].fillna(0) == 0).astype(int)\n",
        "    df_eng['prod_qty_is_one'] = (df_eng['prod_qty'].fillna(0) == 1).astype(int)\n",
        "    # Check if payment doesn't match total (using the calculated check)\n",
        "    # Check for small discrepancies using np.isclose due to potential float issues\n",
        "    df_eng['payment_mismatch_flag'] = (~np.isclose(df_eng['total'].fillna(0), df_eng['payment_check'].fillna(0))).astype(int)\n",
        "\n",
        "\n",
        "    # --- 4. Interaction Features ---\n",
        "    # Example: interaction between annotations\n",
        "    if 'digital annotation' in df_eng.columns and 'handwritten annotation' in df_eng.columns:\n",
        "        df_eng['digital_and_handwritten'] = df_eng['digital annotation'] * df_eng['handwritten annotation']\n",
        "\n",
        "\n",
        "    print(\"  Feature Engineering finished.\")\n",
        "    return df_eng\n"
      ],
      "metadata": {
        "id": "cpw6aOzDG8oX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions (Evaluation & Plotting - similar to previous script) ---\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \"\"\"Evaluates model on train, validation, and test sets.\"\"\"\n",
        "    results = {}\n",
        "    print(\"      Evaluating model...\")\n",
        "    for name, X, y in [('Train', X_train, y_train), ('Validation', X_val, y_val), ('Test', X_test, y_test)]:\n",
        "        if X is None or y is None:\n",
        "            print(f\"      Skipping evaluation for {name} set (data not available).\")\n",
        "            continue\n",
        "        y_pred = model.predict(X)\n",
        "        try:\n",
        "             y_prob = model.predict_proba(X)[:, 1]\n",
        "             roc_auc = roc_auc_score(y, y_prob)\n",
        "        except (AttributeError, NotImplementedError):\n",
        "             y_prob = None\n",
        "             roc_auc = None\n",
        "             # print(f\"      Note: ROC AUC not available for {name} set.\") # Less verbose\n",
        "\n",
        "        results[name] = {\n",
        "            'Accuracy': accuracy_score(y, y_pred),\n",
        "            'Precision': precision_score(y, y_pred, zero_division=0),\n",
        "            'Recall': recall_score(y, y_pred, zero_division=0),\n",
        "            'F1 Score': f1_score(y, y_pred, zero_division=0),\n",
        "            'ROC AUC': roc_auc\n",
        "        }\n",
        "        # Less verbose output during evaluation loop\n",
        "        # print(f\"    {name} Metrics: Acc={results[name]['Accuracy']:.4f}, P={results[name]['Precision']:.4f}, R={results[name]['Recall']:.4f}, F1={results[name]['F1 Score']:.4f}, ROC AUC={results[name]['ROC AUC'] if roc_auc is not None else 'N/A'}\")\n",
        "    print(\"      Evaluation complete.\")\n",
        "    return results\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name, stage, save_dir):\n",
        "    \"\"\"Plots and saves the confusion matrix.\"\"\"\n",
        "    try:\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
        "        ax.set_title(f'{model_name} - Confusion Matrix ({stage} Set)')\n",
        "        plt.tight_layout()\n",
        "        filepath = save_dir / f\"{model_name}_confusion_matrix_{stage.lower()}.png\"\n",
        "        plt.savefig(filepath)\n",
        "        # print(f\"      Saved confusion matrix to {filepath}\")\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"      [ERROR] Failed to plot confusion matrix for {model_name} ({stage}): {e}\")\n",
        "\n",
        "\n",
        "def plot_feature_importance(model, feature_names, model_name, save_dir, top_n=25):\n",
        "    \"\"\"Plots and saves top_n feature importances for tree-based models.\"\"\"\n",
        "    if not hasattr(model, 'feature_importances_'):\n",
        "         print(f\"      Feature importance not available for {model_name}.\")\n",
        "         return\n",
        "\n",
        "    try:\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        top_indices = indices[:top_n]\n",
        "\n",
        "        plt.figure(figsize=(10, max(6, len(top_indices) // 2)))\n",
        "        plt.title(f\"{model_name} - Top {top_n} Feature Importance\")\n",
        "        plt.barh(range(len(top_indices)), importances[top_indices], align='center')\n",
        "        plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices])\n",
        "        plt.xlabel('Importance')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        filepath = save_dir / f\"{model_name}_feature_importance.png\"\n",
        "        plt.savefig(filepath)\n",
        "        # print(f\"      Saved feature importance plot to {filepath}\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "         print(f\"      [ERROR] Failed to plot feature importance for {model_name}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CIgIb9SQG8fY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Script ---\n",
        "\n",
        "# 1. Load Data\n",
        "print(\"--- Loading Data ---\")\n",
        "try:\n",
        "    df_train_raw = pd.read_csv(TRAIN_FILE, low_memory=False) # Use low_memory=False if mixed types cause issues\n",
        "    df_val_raw = pd.read_csv(VAL_FILE, low_memory=False)\n",
        "    df_test_raw = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "    print(f\"Raw data loaded: Train={df_train_raw.shape}, Val={df_val_raw.shape}, Test={df_test_raw.shape}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"[FATAL ERROR] Could not load data files: {e}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"[FATAL ERROR] Error loading data: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Convert date/datetime columns *before* feature engineering\n",
        "print(\"\\n--- Converting Datetime Columns ---\")\n",
        "for df in [df_train_raw, df_val_raw, df_test_raw]:\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce') # Handles NaT\n",
        "    # Assuming 'datetime' was created correctly before, ensure it's dt type\n",
        "    if 'datetime' in df.columns:\n",
        "         df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RKFR8jPrG8WG",
        "outputId": "21862a07-d7ba-4884-ab48-8ddfa3210492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data ---\n",
            "Raw data loaded: Train=(1703, 22), Val=(499, 22), Test=(604, 22)\n",
            "\n",
            "--- Converting Datetime Columns ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 2. Feature Engineering\n",
        "print(\"\\n--- Feature Engineering ---\")\n",
        "df_train_eng = feature_engineer(df_train_raw)\n",
        "df_val_eng = feature_engineer(df_val_raw)\n",
        "df_test_eng = feature_engineer(df_test_raw)\n",
        "\n",
        "# Define FINAL feature list after engineering\n",
        "engineered_features = [\n",
        "    # Datetime basic\n",
        "    'hour', 'minute', 'day_of_week', 'day_of_year', 'month', 'year', 'is_weekend',\n",
        "    # Datetime cyclical\n",
        "    'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos',\n",
        "    # Financial Calculations\n",
        "    'prod_calc_diff', 'payment_check', 'payment_vs_total_diff',\n",
        "    'sum_prod_amt_per_invoice', 'product_count_per_invoice', 'total_vs_sum_prod_diff',\n",
        "    # Ratios\n",
        "    'tax_ratio', 'discount_ratio', 'change_ratio',\n",
        "    # Indicators\n",
        "    'is_discount_applied', 'is_tax_applied', 'is_change_given',\n",
        "    'prod_price_is_zero', 'prod_qty_is_one', 'payment_mismatch_flag',\n",
        "    # Interactions\n",
        "    'digital_and_handwritten'\n",
        "]\n",
        "\n",
        "# Combine base features with newly engineered ones (only those that actually exist)\n",
        "FINAL_FEATURE_COLUMNS = BASE_FEATURES + [feat for feat in engineered_features if feat in df_train_eng.columns]\n",
        "# Remove duplicates just in case\n",
        "FINAL_FEATURE_COLUMNS = sorted(list(set(FINAL_FEATURE_COLUMNS)))\n",
        "\n",
        "print(f\"\\nFinal features selected ({len(FINAL_FEATURE_COLUMNS)}): {', '.join(FINAL_FEATURE_COLUMNS)}\")\n",
        "\n",
        "# 3. Preprocessing (Imputation & Scaling on FINAL features)\n",
        "print(\"\\n--- Preprocessing Final Features ---\")\n",
        "\n",
        "# Select final features and target\n",
        "X_train_eng = df_train_eng[FINAL_FEATURE_COLUMNS].copy()\n",
        "y_train = df_train_eng[TARGET_COLUMN].copy().astype(int)\n",
        "\n",
        "X_val_eng = df_val_eng[FINAL_FEATURE_COLUMNS].copy()\n",
        "y_val = df_val_eng[TARGET_COLUMN].copy().astype(int)\n",
        "\n",
        "X_test_eng = df_test_eng[FINAL_FEATURE_COLUMNS].copy()\n",
        "y_test = df_test_eng[TARGET_COLUMN].copy().astype(int)\n",
        "\n",
        "\n",
        "# Imputation (fit on train only)\n",
        "print(\"  Fitting imputer (strategy='median')...\")\n",
        "imputer = SimpleImputer(strategy='median', keep_empty_features=False) # keep_empty_features for newer sklearn\n",
        "try:\n",
        "    X_train_imputed = imputer.fit_transform(X_train_eng)\n",
        "    X_val_imputed = imputer.transform(X_val_eng)\n",
        "    X_test_imputed = imputer.transform(X_test_eng)\n",
        "    print(\"  Imputation successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"  [ERROR] Imputation failed: {e}. Check for non-numeric data or all-NaN columns in FINAL_FEATURE_COLUMNS.\")\n",
        "    # Fallback or debug: print columns with issues\n",
        "    # print(X_train_eng.dtypes)\n",
        "    # print(X_train_eng.isna().sum())\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Scaling (fit on train only)\n",
        "print(\"  Fitting scaler (StandardScaler)...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "print(\"  Scaling successful.\")\n",
        "\n",
        "\n",
        "# Convert back to DataFrames for easier handling later (optional but good for feature names)\n",
        "X_train_processed = pd.DataFrame(X_train_scaled, columns=FINAL_FEATURE_COLUMNS)\n",
        "X_val_processed = pd.DataFrame(X_val_scaled, columns=FINAL_FEATURE_COLUMNS)\n",
        "X_test_processed = pd.DataFrame(X_test_scaled, columns=FINAL_FEATURE_COLUMNS)\n",
        "\n",
        "# Check for issues after final preprocessing\n",
        "print(f\"\\nShapes after preprocessing: X_train={X_train_processed.shape}, X_val={X_val_processed.shape}, X_test={X_test_processed.shape}\")\n",
        "print(f\"NaNs in X_train: {X_train_processed.isna().sum().sum()}, X_val: {X_val_processed.isna().sum().sum()}, X_test: {X_test_processed.isna().sum().sum()}\")\n"
      ],
      "metadata": {
        "id": "7gbJBSxcH4AO",
        "outputId": "5828ded0-c89d-4e39-e650-04530a0e3184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Feature Engineering ---\n",
            "  Starting Feature Engineering...\n",
            "    Extracting datetime features...\n",
            "    Applying cyclical encoding...\n",
            "    Calculating financial features...\n",
            "    Calculating group-wise features (per file_name)...\n",
            "    Calculating financial ratios...\n",
            "    Creating indicator flags...\n",
            "  Feature Engineering finished.\n",
            "  Starting Feature Engineering...\n",
            "    Extracting datetime features...\n",
            "    Applying cyclical encoding...\n",
            "    Calculating financial features...\n",
            "    Calculating group-wise features (per file_name)...\n",
            "    Calculating financial ratios...\n",
            "    Creating indicator flags...\n",
            "  Feature Engineering finished.\n",
            "  Starting Feature Engineering...\n",
            "    Extracting datetime features...\n",
            "    Applying cyclical encoding...\n",
            "    Calculating financial features...\n",
            "    Calculating group-wise features (per file_name)...\n",
            "    Calculating financial ratios...\n",
            "    Creating indicator flags...\n",
            "  Feature Engineering finished.\n",
            "\n",
            "Final features selected (39): amt_paid, change, change_ratio, day_of_week, day_of_week_cos, day_of_week_sin, day_of_year, digital annotation, digital_and_handwritten, discount, discount_ratio, handwritten annotation, hour, hour_cos, hour_sin, is_change_given, is_discount_applied, is_tax_applied, is_weekend, minute, month, month_cos, month_sin, payment_check, payment_mismatch_flag, payment_vs_total_diff, prod_amt, prod_calc_diff, prod_price, prod_price_is_zero, prod_qty, prod_qty_is_one, product_count_per_invoice, sum_prod_amt_per_invoice, tax, tax_ratio, total, total_vs_sum_prod_diff, year\n",
            "\n",
            "--- Preprocessing Final Features ---\n",
            "  Fitting imputer (strategy='median')...\n",
            "  Imputation successful.\n",
            "  Fitting scaler (StandardScaler)...\n",
            "  Scaling successful.\n",
            "\n",
            "Shapes after preprocessing: X_train=(1703, 39), X_val=(499, 39), X_test=(604, 39)\n",
            "NaNs in X_train: 0, X_val: 0, X_test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Import cuML's GPU-enabled GridSearchCV and estimators\n",
        "from cuml.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold  # still use scikit-learn CV splitter\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, roc_curve)\n",
        "import xgboost as xgb\n",
        "\n",
        "# GPU estimators from cuML (if available)\n",
        "from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
        "from cuml.svm import SVC as cuSVC\n",
        "from cuml.ensemble import RandomForestClassifier as cuRFClassifier\n",
        "\n",
        "# For models that cuML does not support, we can still use scikit-learn's version.\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Ignore some warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Assume that X_train_processed, y_train, X_val_processed, y_val, X_test_processed, y_test\n",
        "# and OUTPUT_PLOTS_DIR, OUTPUT_MODELS_DIR, OUTPUT_RESULTS_DIR are already defined.\n",
        "# Also assume that evaluate_model, plot_confusion_matrix, plot_feature_importance are defined.\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Model Training and Evaluation Loop ---\")\n",
        "\n",
        "# Calculate scale_pos_weight for XGBoost (remains the same)\n",
        "pos_weight = (y_train == 0).sum() / (y_train == 1).sum() if (y_train == 1).sum() > 0 else 1\n",
        "print(f\"Calculated scale_pos_weight for XGBoost: {pos_weight:.2f}\")\n",
        "\n",
        "# Define models and parameter grids.\n",
        "# NOTE: We switch to GPU-enabled estimators where possible and adjust parameter grids.\n",
        "models = {\n",
        "    'LogisticRegression': (\n",
        "        # Remove random_state because cuML ignores it.\n",
        "        cuLogisticRegression(solver='qn', max_iter=1000),\n",
        "        {\n",
        "            'C': [0.01, 0.1, 1, 10, 100],\n",
        "            # cuML's logistic regression currently supports only l2 penalty.\n",
        "            'penalty': ['l2']\n",
        "        }\n",
        "    ),\n",
        "    'SVC': (\n",
        "        cuSVC(probability=True),\n",
        "        {\n",
        "            'C': [0.1, 1, 10, 50],\n",
        "            'gamma': ['scale', 'auto', 0.01, 0.1],\n",
        "            # cuML's SVC currently supports only rbf kernel\n",
        "            'kernel': ['rbf']\n",
        "        }\n",
        "    ),\n",
        "    'RandomForestClassifier': (\n",
        "        # Adjust parameter grid for cuML's RandomForestClassifier:\n",
        "        cuRFClassifier(),\n",
        "        {\n",
        "            'n_estimators': [100, 200],\n",
        "            # Remove None from max_depth because cuML expects an integer.\n",
        "            'max_depth': [10, 20, 30],\n",
        "            # Remove parameters not (yet) supported by cuML (e.g., min_samples_split).\n",
        "            'max_features': ['sqrt', 'log2']\n",
        "        }\n",
        "    ),\n",
        "    'DecisionTreeClassifier': (\n",
        "        DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
        "        {\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30, 50],\n",
        "            'min_samples_split': [2, 5, 10, 20],\n",
        "            'min_samples_leaf': [1, 3, 5, 10]\n",
        "        }\n",
        "    ),\n",
        "    'XGBClassifier': (\n",
        "        xgb.XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False,\n",
        "            random_state=42,\n",
        "            scale_pos_weight=pos_weight,\n",
        "            n_jobs=-1,\n",
        "            tree_method='gpu_hist',       # Use GPU hist method\n",
        "            predictor='gpu_predictor'      # Use GPU predictor\n",
        "        ),\n",
        "        {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'subsample': [0.8, 0.9],\n",
        "            'colsample_bytree': [0.8, 0.9]\n",
        "        }\n",
        "    )\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "best_models = {}\n",
        "roc_data_test = {}  # To store ROC data for combined ROC plot\n",
        "\n",
        "# Use StratifiedKFold for cross-validation.\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List of model names to train.\n",
        "MODELS_TO_TRAIN = ['LogisticRegression', 'SVC', 'RandomForestClassifier',\n",
        "                     'DecisionTreeClassifier', 'XGBClassifier']\n",
        "\n",
        "# --- Modified evaluate_model function ---\n",
        "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \"\"\" Evaluate the given model on train, validation, and test sets.\n",
        "        Here we ensure that predict_proba outputs are converted to a NumPy array.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for stage, (X, y) in zip(['Train', 'Validation', 'Test'],\n",
        "                              [(X_train, y_train), (X_val, y_val), (X_test, y_test)]):\n",
        "        y_pred = model.predict(X)\n",
        "        # Ensure we have a proper NumPy array from predict_proba.\n",
        "        y_prob = model.predict_proba(X)\n",
        "        if hasattr(y_prob, \"get\"):  # if it's a GPU array (cupy or cuDF), convert it.\n",
        "            y_prob = y_prob.get()\n",
        "        y_prob = np.asarray(y_prob)\n",
        "        # If y_prob has two columns, take the second column.\n",
        "        if y_prob.ndim == 2 and y_prob.shape[1] >= 2:\n",
        "            y_prob = y_prob[:, 1]\n",
        "        results[stage] = {\n",
        "            'Accuracy': accuracy_score(y, y_pred),\n",
        "            'Precision': precision_score(y, y_pred, zero_division=0),\n",
        "            'Recall': recall_score(y, y_pred, zero_division=0),\n",
        "            'F1 Score': f1_score(y, y_pred, zero_division=0),\n",
        "            'ROC AUC': roc_auc_score(y, y_prob)\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Model training loop using GPU-enabled GridSearchCV\n",
        "for name in MODELS_TO_TRAIN:\n",
        "    if name not in models:\n",
        "        print(f\"Model '{name}' not defined. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n===== Training {name} =====\")\n",
        "    model_instance, param_grid = models[name]\n",
        "\n",
        "    # For cuML-based models, the parameter grid must match their supported arguments.\n",
        "    current_param_grid = param_grid\n",
        "\n",
        "    print(f\"  Performing GPU-enabled GridSearchCV (CV={cv.get_n_splits()}, scoring='roc_auc')...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model_instance,\n",
        "        param_grid=current_param_grid,\n",
        "        cv=cv,\n",
        "        scoring='roc_auc',\n",
        "        verbose=1\n",
        "        # Do not set n_jobs; GPU parallelism is handled internally.\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        grid_search.fit(X_train_processed, y_train)\n",
        "        print(f\"\\n  Best Params for {name}: {grid_search.best_params_}\")\n",
        "        print(f\"  Best CV ROC AUC Score: {grid_search.best_score_:.4f}\")\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_models[name] = best_model\n",
        "\n",
        "        print(f\"  Evaluating best {name} model on all sets...\")\n",
        "        model_results = evaluate_model(best_model,\n",
        "                                       X_train_processed, y_train,\n",
        "                                       X_val_processed, y_val,\n",
        "                                       X_test_processed, y_test)\n",
        "        all_results[name] = model_results\n",
        "        print(f\"  Test Set Performance: Acc={model_results['Test']['Accuracy']:.4f}, \"\n",
        "              f\"P={model_results['Test']['Precision']:.4f}, R={model_results['Test']['Recall']:.4f}, \"\n",
        "              f\"F1={model_results['Test']['F1 Score']:.4f}, ROC AUC={model_results['Test']['ROC AUC']:.4f}\")\n",
        "\n",
        "        # Generate plots (using your plotting functions)\n",
        "        print(f\"  Generating plots for {name} (Test Set)...\")\n",
        "        y_pred_test = best_model.predict(X_test_processed)\n",
        "        plot_confusion_matrix(y_test, y_pred_test, name, \"Test\", OUTPUT_PLOTS_DIR)\n",
        "        plot_feature_importance(best_model, FINAL_FEATURE_COLUMNS, name, OUTPUT_PLOTS_DIR)\n",
        "\n",
        "        if model_results['Test']['ROC AUC'] is not None:\n",
        "            y_prob_test = best_model.predict_proba(X_test_processed)\n",
        "            if hasattr(y_prob_test, \"get\"):\n",
        "                y_prob_test = y_prob_test.get()\n",
        "            y_prob_test = np.asarray(y_prob_test)\n",
        "            if y_prob_test.ndim == 2 and y_prob_test.shape[1] >= 2:\n",
        "                fpr, tpr, _ = roc_curve(y_test, y_prob_test[:, 1])\n",
        "                roc_data_test[name] = {'fpr': fpr, 'tpr': tpr, 'auc': model_results['Test']['ROC AUC']}\n",
        "\n",
        "        model_filename = OUTPUT_MODELS_DIR / f\"{name}_best_model.joblib\"\n",
        "        joblib.dump(best_model, model_filename)\n",
        "        print(f\"  Saved best model to {model_filename}\")\n",
        "\n",
        "    except Exception as train_err:\n",
        "        print(f\"  [ERROR] Failed to tune, train or evaluate {name}: {train_err}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Save Results Summary\n",
        "print(\"\\n--- Saving Results Summary ---\")\n",
        "results_df_list = []\n",
        "for model_name, stages in all_results.items():\n",
        "    for stage_name, metrics in stages.items():\n",
        "        metrics_copy = metrics.copy()  # Avoid modifying original dict\n",
        "        metrics_copy['Model'] = model_name\n",
        "        metrics_copy['Stage'] = stage_name\n",
        "        results_df_list.append(metrics_copy)\n",
        "\n",
        "if results_df_list:\n",
        "    results_df = pd.DataFrame(results_df_list)\n",
        "    cols_order = ['Model', 'Stage', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
        "    results_df = results_df[[col for col in cols_order if col in results_df.columns]]\n",
        "    results_filepath = OUTPUT_RESULTS_DIR / \"all_models_evaluation_summary_v2.csv\"\n",
        "    try:\n",
        "        results_df.to_csv(results_filepath, index=False)\n",
        "        print(f\"Evaluation summary saved to {results_filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not save results summary: {e}\")\n",
        "else:\n",
        "    print(\"No model results generated to save.\")\n"
      ],
      "metadata": {
        "id": "n8pcfOhpH33i",
        "outputId": "ef921499-fb45-4c43-c8fd-b8dd4dc3c5aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Training and Evaluation Loop ---\n",
            "Calculated scale_pos_weight for XGBoost: 4.75\n",
            "\n",
            "===== Training LogisticRegression =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "\n",
            "  Best Params for LogisticRegression: {'C': 0.01, 'penalty': 'l2'}\n",
            "  Best CV ROC AUC Score: nan\n",
            "  Evaluating best LogisticRegression model on all sets...\n",
            "  [ERROR] Failed to tune, train or evaluate LogisticRegression: NDFrame.get() missing 1 required positional argument: 'key'\n",
            "\n",
            "===== Training SVC =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-10-2c7d7b04a2c7>\", line 173, in <cell line: 0>\n",
            "    model_results = evaluate_model(best_model,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-2c7d7b04a2c7>\", line 128, in evaluate_model\n",
            "    y_prob = y_prob.get()\n",
            "             ^^^^^^^^^^^^\n",
            "TypeError: NDFrame.get() missing 1 required positional argument: 'key'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Best Params for SVC: {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "  Best CV ROC AUC Score: nan\n",
            "  Evaluating best SVC model on all sets...\n",
            "  [ERROR] Failed to tune, train or evaluate SVC: NDFrame.get() missing 1 required positional argument: 'key'\n",
            "\n",
            "===== Training RandomForestClassifier =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-10-2c7d7b04a2c7>\", line 173, in <cell line: 0>\n",
            "    model_results = evaluate_model(best_model,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-2c7d7b04a2c7>\", line 128, in evaluate_model\n",
            "    y_prob = y_prob.get()\n",
            "             ^^^^^^^^^^^^\n",
            "TypeError: NDFrame.get() missing 1 required positional argument: 'key'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Best Params for RandomForestClassifier: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
            "  Best CV ROC AUC Score: nan\n",
            "  Evaluating best RandomForestClassifier model on all sets...\n",
            "  [ERROR] Failed to tune, train or evaluate RandomForestClassifier: NDFrame.get() missing 1 required positional argument: 'key'\n",
            "\n",
            "===== Training DecisionTreeClassifier =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-10-2c7d7b04a2c7>\", line 173, in <cell line: 0>\n",
            "    model_results = evaluate_model(best_model,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-2c7d7b04a2c7>\", line 128, in evaluate_model\n",
            "    y_prob = y_prob.get()\n",
            "             ^^^^^^^^^^^^\n",
            "TypeError: NDFrame.get() missing 1 required positional argument: 'key'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Best Params for DecisionTreeClassifier: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
            "  Best CV ROC AUC Score: 0.9344\n",
            "  Evaluating best DecisionTreeClassifier model on all sets...\n",
            "  Test Set Performance: Acc=0.6854, P=0.1351, R=0.1376, F1=0.1364, ROC AUC=0.4285\n",
            "  Generating plots for DecisionTreeClassifier (Test Set)...\n",
            "  Saved best model to /content/drive/MyDrive/classification_results_v2/models/DecisionTreeClassifier_best_model.joblib\n",
            "\n",
            "===== Training XGBClassifier =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "\n",
            "  Best Params for XGBClassifier: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.9}\n",
            "  Best CV ROC AUC Score: 0.9770\n",
            "  Evaluating best XGBClassifier model on all sets...\n",
            "  Test Set Performance: Acc=0.7930, P=0.3400, R=0.1560, F1=0.2138, ROC AUC=0.4489\n",
            "  Generating plots for XGBClassifier (Test Set)...\n",
            "  Saved best model to /content/drive/MyDrive/classification_results_v2/models/XGBClassifier_best_model.joblib\n",
            "\n",
            "--- Saving Results Summary ---\n",
            "Evaluation summary saved to /content/drive/MyDrive/classification_results_v2/all_models_evaluation_summary_v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# Attempt to import cupy, if available.\n",
        "try:\n",
        "    import cupy as cp\n",
        "except ImportError:\n",
        "    cp = None\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the given model on train, validation, and test sets.\n",
        "    Convert model.predict_proba outputs to a NumPy array if necessary.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for stage, (X, y) in zip(['Train', 'Validation', 'Test'],\n",
        "                              [(X_train, y_train), (X_val, y_val), (X_test, y_test)]):\n",
        "        y_pred = model.predict(X)\n",
        "        # Obtain probability estimates.\n",
        "        y_prob = model.predict_proba(X)\n",
        "        # Convert GPU arrays (CuPy) to NumPy, or cuDF/pandas objects via to_numpy().\n",
        "        if cp is not None and isinstance(y_prob, cp.ndarray):\n",
        "            y_prob = y_prob.get()\n",
        "        elif hasattr(y_prob, \"to_numpy\"):\n",
        "            y_prob = y_prob.to_numpy()\n",
        "        else:\n",
        "            y_prob = np.asarray(y_prob)\n",
        "        # If y_prob has two columns, select the probability of the positive class.\n",
        "        if y_prob.ndim == 2 and y_prob.shape[1] >= 2:\n",
        "            y_prob = y_prob[:, 1]\n",
        "        results[stage] = {\n",
        "            'Accuracy':      np.round(accuracy_score(y, y_pred), 4),\n",
        "            'Precision':     np.round(precision_score(y, y_pred, zero_division=0), 4),\n",
        "            'Recall':        np.round(recall_score(y, y_pred, zero_division=0), 4),\n",
        "            'F1 Score':      np.round(f1_score(y, y_pred, zero_division=0), 4),\n",
        "            'ROC AUC':       np.round(roc_auc_score(y, y_prob), 4)\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# Model training loop (GPU-enabled GridSearchCV with cuml, etc.)\n",
        "# In your training loop, simply call evaluate_model as before.\n",
        "for name in MODELS_TO_TRAIN:\n",
        "    if name not in models:\n",
        "        print(f\"Model '{name}' not defined. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n===== Training {name} =====\")\n",
        "    model_instance, param_grid = models[name]\n",
        "    current_param_grid = param_grid\n",
        "\n",
        "    print(f\"  Performing GPU-enabled GridSearchCV (CV={cv.get_n_splits()}, scoring='roc_auc')...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model_instance,\n",
        "        param_grid=current_param_grid,\n",
        "        cv=cv,\n",
        "        scoring='roc_auc',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        grid_search.fit(X_train_processed, y_train)\n",
        "        print(f\"\\n  Best Params for {name}: {grid_search.best_params_}\")\n",
        "        print(f\"  Best CV ROC AUC Score: {grid_search.best_score_:.4f}\")\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_models[name] = best_model\n",
        "\n",
        "        print(f\"  Evaluating best {name} model on all sets...\")\n",
        "        model_results = evaluate_model(\n",
        "            best_model,\n",
        "            X_train_processed, y_train,\n",
        "            X_val_processed, y_val,\n",
        "            X_test_processed, y_test\n",
        "        )\n",
        "        all_results[name] = model_results\n",
        "        print(f\"  Test Set Performance: Acc={model_results['Test']['Accuracy']:.4f}, \"\n",
        "              f\"P={model_results['Test']['Precision']:.4f}, R={model_results['Test']['Recall']:.4f}, \"\n",
        "              f\"F1={model_results['Test']['F1 Score']:.4f}, ROC AUC={model_results['Test']['ROC AUC']:.4f}\")\n",
        "\n",
        "        # ---------------------- Generate and Save Plots ----------------------\n",
        "        print(f\"  Generating plots for {name} (Test Set)...\")\n",
        "        # 1. Confusion Matrix plot (assuming plot_confusion_matrix is defined)\n",
        "        y_pred_test = best_model.predict(X_test_processed)\n",
        "        plot_confusion_matrix(y_test, y_pred_test, name, \"Test\", OUTPUT_PLOTS_DIR)\n",
        "\n",
        "        # 2. Feature Importance plot (assuming your function handles GPU models as needed)\n",
        "        plot_feature_importance(best_model, FINAL_FEATURE_COLUMNS, name, OUTPUT_PLOTS_DIR)\n",
        "\n",
        "        # 3. ROC Curve plot\n",
        "        y_prob_test = best_model.predict_proba(X_test_processed)\n",
        "        if cp is not None and isinstance(y_prob_test, cp.ndarray):\n",
        "            y_prob_test = y_prob_test.get()\n",
        "        elif hasattr(y_prob_test, \"to_numpy\"):\n",
        "            y_prob_test = y_prob_test.to_numpy()\n",
        "        y_prob_test = np.asarray(y_prob_test)\n",
        "        if y_prob_test.ndim == 2 and y_prob_test.shape[1] >= 2:\n",
        "            fpr, tpr, thresholds = roc_curve(y_test, y_prob_test[:, 1])\n",
        "            plt.figure()\n",
        "            plt.plot(fpr, tpr, label=f'{name} (AUC = {model_results[\"Test\"][\"ROC AUC\"]:.2f})')\n",
        "            plt.plot([0, 1], [0, 1], 'k--')\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'ROC Curve - {name} (Test Set)')\n",
        "            plt.legend(loc='lower right')\n",
        "            roc_plot_path = OUTPUT_PLOTS_DIR / f\"{name}_roc_curve.png\"\n",
        "            plt.savefig(roc_plot_path)\n",
        "            plt.close()\n",
        "            print(f\"  Saved ROC curve plot to {roc_plot_path}\")\n",
        "\n",
        "        # ---------------------- Save Model ----------------------\n",
        "        model_filename = OUTPUT_MODELS_DIR / f\"{name}_best_model.joblib\"\n",
        "        joblib.dump(best_model, model_filename)\n",
        "        print(f\"  Saved best model to {model_filename}\")\n",
        "\n",
        "    except Exception as train_err:\n",
        "        print(f\"  [ERROR] Failed to tune, train or evaluate {name}: {train_err}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "RZPWL0EZUu_R",
        "outputId": "20b416e6-bdf2-4a2a-9b5d-da41a74c5c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Training LogisticRegression =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "\n",
            "  Best Params for LogisticRegression: {'C': 0.01, 'penalty': 'l2'}\n",
            "  Best CV ROC AUC Score: nan\n",
            "  Evaluating best LogisticRegression model on all sets...\n",
            "  Test Set Performance: Acc=0.7616, P=0.0732, R=0.0275, F1=0.0400, ROC AUC=0.4560\n",
            "  Generating plots for LogisticRegression (Test Set)...\n",
            "      Feature importance not available for LogisticRegression.\n",
            "  Saved ROC curve plot to /content/drive/MyDrive/classification_results_v2/plots/LogisticRegression_roc_curve.png\n",
            "  Saved best model to /content/drive/MyDrive/classification_results_v2/models/LogisticRegression_best_model.joblib\n",
            "\n",
            "===== Training SVC =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "  Best Params for SVC: {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "  Best CV ROC AUC Score: nan\n",
            "  Evaluating best SVC model on all sets...\n",
            "  Test Set Performance: Acc=0.8162, P=0.0000, R=0.0000, F1=0.0000, ROC AUC=0.5423\n",
            "  Generating plots for SVC (Test Set)...\n",
            "      Feature importance not available for SVC.\n",
            "  Saved ROC curve plot to /content/drive/MyDrive/classification_results_v2/plots/SVC_roc_curve.png\n",
            "  Saved best model to /content/drive/MyDrive/classification_results_v2/models/SVC_best_model.joblib\n",
            "\n",
            "===== Training RandomForestClassifier =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "\n",
            "  Best Params for RandomForestClassifier: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
            "  Best CV ROC AUC Score: nan\n",
            "  Evaluating best RandomForestClassifier model on all sets...\n",
            "  Test Set Performance: Acc=0.8179, P=0.4444, R=0.0367, F1=0.0678, ROC AUC=0.5394\n",
            "  Generating plots for RandomForestClassifier (Test Set)...\n",
            "      Feature importance not available for RandomForestClassifier.\n",
            "  Saved ROC curve plot to /content/drive/MyDrive/classification_results_v2/plots/RandomForestClassifier_roc_curve.png\n",
            "  Saved best model to /content/drive/MyDrive/classification_results_v2/models/RandomForestClassifier_best_model.joblib\n",
            "\n",
            "===== Training DecisionTreeClassifier =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
            "\n",
            "  Best Params for DecisionTreeClassifier: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
            "  Best CV ROC AUC Score: 0.9344\n",
            "  Evaluating best DecisionTreeClassifier model on all sets...\n",
            "  Test Set Performance: Acc=0.6854, P=0.1351, R=0.1376, F1=0.1364, ROC AUC=0.4285\n",
            "  Generating plots for DecisionTreeClassifier (Test Set)...\n",
            "  Saved ROC curve plot to /content/drive/MyDrive/classification_results_v2/plots/DecisionTreeClassifier_roc_curve.png\n",
            "  Saved best model to /content/drive/MyDrive/classification_results_v2/models/DecisionTreeClassifier_best_model.joblib\n",
            "\n",
            "===== Training XGBClassifier =====\n",
            "  Performing GPU-enabled GridSearchCV (CV=5, scoring='roc_auc')...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "\n",
            "  Best Params for XGBClassifier: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.9}\n",
            "  Best CV ROC AUC Score: 0.9770\n",
            "  Evaluating best XGBClassifier model on all sets...\n",
            "  Test Set Performance: Acc=0.7930, P=0.3400, R=0.1560, F1=0.2138, ROC AUC=0.4489\n",
            "  Generating plots for XGBClassifier (Test Set)...\n",
            "  Saved ROC curve plot to /content/drive/MyDrive/classification_results_v2/plots/XGBClassifier_roc_curve.png\n",
            "  Saved best model to /content/drive/MyDrive/classification_results_v2/models/XGBClassifier_best_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Combined ROC Curve Plot\n",
        "print(\"\\n--- Generating Combined ROC Curve Plot ---\")\n",
        "# (Keep ROC plotting code same as before)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_count = 0\n",
        "for name, data in roc_data_test.items():\n",
        "    if data.get('auc') is not None:\n",
        "        plt.plot(data['fpr'], data['tpr'], lw=2, label=f\"{name} (AUC = {data['auc']:.3f})\")\n",
        "        plot_count += 1\n",
        "\n",
        "if plot_count > 0:\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Chance Level (AUC = 0.500)') # Diagonal line\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves - Test Set Comparison')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    roc_comp_filepath = OUTPUT_PLOTS_DIR / \"combined_roc_curves_test_set_v2.png\"\n",
        "    try:\n",
        "        plt.savefig(roc_comp_filepath)\n",
        "        print(f\"Combined ROC plot saved to {roc_comp_filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not save combined ROC plot: {e}\")\n",
        "else:\n",
        "    print(\"No models with valid ROC AUC data to plot.\")\n",
        "\n",
        "plt.close()\n",
        "\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")"
      ],
      "metadata": {
        "id": "kcGmKsb2H6Kv",
        "outputId": "87dcbc19-b457-44bf-d262-727754419e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Combined ROC Curve Plot ---\n",
            "Combined ROC plot saved to /content/drive/MyDrive/classification_results_v2/plots/combined_roc_curves_test_set_v2.png\n",
            "\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chp3sWocH6IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDBIH6TcH6Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPrkJ_REH6DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cugraph\n",
        "cugraph.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "JOCMWaUal1fI",
        "outputId": "70c09cea-15c4-46e5-b44b-40ff7aa55d6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cugraph'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-be8ca0ce684f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcugraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcugraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cugraph'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cugraph"
      ],
      "metadata": {
        "id": "l7ZDXnOQA6J2",
        "outputId": "07f9e2ae-45f5-4afc-d258-61e4b7a3b14f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cugraph\n",
            "  Downloading cugraph-0.6.1.post1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: cugraph\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for cugraph (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for cugraph\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for cugraph\n",
            "Failed to build cugraph\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (cugraph)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuspatial\n",
        "cuspatial.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "AnmtYjzvVTtv",
        "outputId": "9af7ed26-779d-4f66-ff58-6a0c679cd4fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cuspatial'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e647765eb68a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcuspatial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcuspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuspatial'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuxfilter\n",
        "cuxfilter.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "CYjcARDFVWWD",
        "outputId": "297e9b0d-1ab4-40d7-8042-49f4828e18c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cuxfilter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6928215a52a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcuxfilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcuxfilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuxfilter'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlsyk9m9NN2K"
      },
      "source": [
        "# Next Steps #\n",
        "\n",
        "For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n",
        "\n",
        "For more RAPIDS examples, check out our RAPIDS notebooks repos:\n",
        "1. https://github.com/rapidsai/notebooks\n",
        "2. https://github.com/rapidsai/notebooks-contrib"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9GE3Jvj8d3_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}