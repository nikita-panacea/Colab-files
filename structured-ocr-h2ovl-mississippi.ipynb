{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11192869,"sourceType":"datasetVersion","datasetId":6987511}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nikita2998/structured-ocr-h2ovl-mississippi?scriptVersionId=231366784\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:09:52.713925Z","iopub.execute_input":"2025-04-02T09:09:52.714156Z","iopub.status.idle":"2025-04-02T09:09:56.921424Z","shell.execute_reply.started":"2025-04-02T09:09:52.714135Z","shell.execute_reply":"2025-04-02T09:09:56.920391Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install einops timm peft sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:09:56.922593Z","iopub.execute_input":"2025-04-02T09:09:56.922916Z","iopub.status.idle":"2025-04-02T09:10:00.368215Z","shell.execute_reply.started":"2025-04-02T09:09:56.922885Z","shell.execute_reply":"2025-04-02T09:10:00.367392Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.29.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer\nimport os\nos.environ[\"FLASH_ATTN_DISABLE\"] = \"1\"\n\n# Set up the model and tokenizer\nmodel_path = 'h2oai/h2ovl-mississippi-2b'\nmodel = AutoModel.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True).eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n\n\n# # pure-text conversation\n# question = 'Hello, who are you?'\n# response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n# print(f'User: {question}\\nAssistant: {response}')\n\n\n# Example for single image\nimage_file = '/kaggle/input/find-it-again-dataset/findit2/train/X51005200938.png'\nquestion = \"\"\"<image>\\n\nPlease extract the following information in JSON format:\n- Invoice Number\n- Company Name\n- Address\n- Contact Information (Telephone, Email)\n- Date (DD/MM/YYYY format)\n- Time (HH:MM:SS format)\n- Product Details (for each product: Name, ID, Price, Quantity, Total Amount)\n- Total Amount\n- Cash/Amount Paid\n- Change\n- Tax/GST\n- Discount\nDo not include json tags like ```json or ``` in the output. Just return the json object.\nIf data for a field is not present in the image leave it blank with empty string \"\".\nAlways return the output as a valid JSON object with keys corresponding to these fields.\n\"\"\"\nresponse, history = model.chat(tokenizer, image_file, question, generation_config, history=None, return_history=True)\nprint(f'Assistant: {response}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T16:27:56.325093Z","iopub.execute_input":"2025-04-02T16:27:56.325296Z","iopub.status.idle":"2025-04-02T16:29:11.740703Z","shell.execute_reply.started":"2025-04-02T16:27:56.325276Z","shell.execute_reply":"2025-04-02T16:29:11.739773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e33375a13fc4230b63c0204fe33dc8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_h2ovl_chat.py:   0%|          | 0.00/3.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ad6a1719e44ff1bb0d482c360236ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_intern_vit.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a4ae6dc6864d7f9913e0e4e2189ac7"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- configuration_h2ovl_chat.py\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_h2ovl_chat.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1114dd04724ee9adc942ba823933e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversation.py:   0%|          | 0.00/5.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970655d024bf4243a634f3f1acf682ba"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"image_process.py:   0%|          | 0.00/6.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7838903508240c3881d17e9f11054b5"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- image_process.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_intern_vit.py:   0%|          | 0.00/18.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0862c95c793d47f48878acf4a1a27e40"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- modeling_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- modeling_h2ovl_chat.py\n- conversation.py\n- image_process.py\n- modeling_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"FlashAttention is not installed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab98fbca24e4138b3192a062920085b"}},"metadata":{}},{"name":"stdout","text":"Warning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/161 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11254bd8f9554e9bbabf647706901c37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fda961f42484818affffe72f029fc2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c12eb957b94e3086d827fdce4f6e40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/199 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d492cbf38af4ae8835a11bfdca9033a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5b127dd5f74b91a099bc39200bd3dc"}},"metadata":{}},{"name":"stdout","text":"Assistant: {\n    \"Invoice Number\": \"A02045\",\n    \"Company Name\": \"PERNIAGAAN ZHENG HUI\",\n    \"Address\": \"JM032895-V\\nNO. 69 PALMAHARAU PJ PRAMAS JAYA\\n187600 JOHOR BAHU\\nBANDAR BARU PERMAS JAYA\",\n    \"Contact Information\": {\n        \"Telephone\": \"07-348 7524\",\n        \"Email\": \"GST NO: 000800589624\"\n    },\n    \"Date\": \"12/02/2018\",\n    \"Time\": \"08:30:30\",\n    \"Product Details\": [\n        {\n            \"Name\": \"SRR. 11\\\" PROWESH H/DUTY SILICONE GUNJ G-D2\",\n            \"ID\": 8239,\n            \"Price\": 16.00,\n            \"Quantity\": 1,\n            \"Total Amount\": 16.00\n        },\n        {\n            \"Name\": \"SRR. XTRASEAL RTV ACETIC SILICONE SA-107\",\n            \"ID\": 9552075109147,\n            \"Price\": 7.00,\n            \"Quantity\": 3,\n            \"Total Amount\": 21.00\n        }\n    ],\n    \"Total Amount\": 112.45,\n    \"Cash/Amount Paid\": 112.45,\n    \"Change\": 0.00,\n    \"Tax/GST\": 6.37,\n    \"Discount\": 0.00,\n    \"GST Summary\": {\n        \"Tax Code\": 6,\n        \"Percentage\": 6.10,\n        \"Amount GST\": 6.37\n    }\n}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!apt install g++","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install flash-attn==2.7.3 --no-build-isolation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport json\nimport torch\nimport logging\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm import tqdm  # for progress bar\n\n# Disable FLASH_ATTN if needed\nos.environ[\"FLASH_ATTN_DISABLE\"] = \"1\"\n\n# -------------------------------\n# 1. Set up logging configuration\n# -------------------------------\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\n\n# -------------------------------\n# 2. Set up the model and tokenizer for GPU\n# -------------------------------\nmodel_path = 'h2oai/h2ovl-mississippi-2b'\nmodel = AutoModel.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).eval().cuda()  # Running on GPU\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n\n# -------------------------------\n# 3. Define the prompt template for structured extraction\n# -------------------------------\nprompt = (\"\"\"<image>\\n\n<image>\\n\nPlease extract the following information in JSON format:\n- Invoice Number\n- Company Name\n- Address\n- Contact Information (Telephone, Email)\n- Date (DD/MM/YYYY format)\n- Time (HH:MM:SS format)\n- Product Details (for each product: Name, ID, Price, Quantity, Total Amount)\n- Total Amount\n- Cash/Amount Paid\n- Change\n- Tax/GST\n- Discount\nDo not include json tags like ```json or ``` in the output. Just return the json object.\nIf data for a field is not present in the image leave it blank with empty string \"\".\nAlways return the output as a valid JSON object with keys corresponding to these fields.\n\"\"\")\n\n# -------------------------------\n# 4. Function to process one folder of images in batches and save JSON output to a specified output folder\n# -------------------------------\ndef process_folder(input_folder, output_folder, batch_size=8):\n    image_files = glob.glob(os.path.join(input_folder, \"*.png\"))\n    logger.info(f\"Found {len(image_files)} images in {input_folder}.\")\n    \n    # Process images in batches using tqdm for progress tracking\n    for i in tqdm(range(0, len(image_files), batch_size), desc=f\"Processing {os.path.basename(input_folder)}\"):\n        batch_files = image_files[i:i+batch_size]\n        logger.info(f\"Processing batch: {batch_files}\")\n        try:\n            # Pass the list of image paths to model.chat for batch processing\n            response, history = model.chat(\n                tokenizer, batch_files, prompt, generation_config, history=None, return_history=True\n            )\n        except Exception as e:\n            logger.error(f\"Error processing batch {batch_files}: {e}\")\n            continue\n        \n        # Assume that 'response' is a list of strings corresponding to each image in the batch\n        for file, resp in zip(batch_files, response):\n            logger.info(f\"Raw response for {file}: {resp}\")\n            try:\n                r = resp.strip()\n                if '```json' in r:\n                    r = r.split('```json')[1].split('```')[0].strip()\n                extracted_data = json.loads(r)\n            except Exception as e:\n                logger.warning(f\"Error parsing JSON for {file}: {e}\")\n                extracted_data = {\"extracted_text\": r}\n            \n            base_name = os.path.splitext(os.path.basename(file))[0]\n            output_file = os.path.join(output_folder, f\"{base_name}.json\")\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(extracted_data, f, indent=4)\n            logger.info(f\"Saved extracted data to {output_file}\")\n\n# -------------------------------\n# 5. Process dataset folders: train, val, and test and save outputs accordingly\n# -------------------------------\ndataset_base = \"/kaggle/input/find-it-again-dataset/findit2\"  # Update with your dataset path\noutput_base = \"/kaggle/working/extracted_jsons\"  # Base output folder for JSON files\n\nfor split in [\"val\", \"test\",\"train\"]:\n    input_folder = os.path.join(dataset_base, split)\n    output_folder = os.path.join(output_base, split)\n    if os.path.isdir(input_folder):\n        os.makedirs(output_folder, exist_ok=True)\n        logger.info(f\"\\nProcessing folder: {input_folder}\")\n        process_folder(input_folder, output_folder, batch_size=4)\n    else:\n        logger.error(f\"Folder {input_folder} does not exist.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport json\nimport torch\nimport logging\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\n# -------------------------------\n# 1. Configure environment and logging\n# -------------------------------\nos.environ[\"FLASH_ATTN_DISABLE\"] = \"1\"  # Disable if causing issues\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\n\n# -------------------------------\n# 2. GPU-optimized model setup\n# -------------------------------\nmodel_path = 'h2oai/h2ovl-mississippi-2b'\n\n# Use bfloat16 for better GPU utilization\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nmodel = AutoModel.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).eval().cuda()\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n\n# Optimize generation config\ngeneration_config = {\n    \"max_new_tokens\": 1024,\n    \"do_sample\": True,\n    \"temperature\": 0.1,\n    \"top_p\": 0.9,\n    \"pad_token_id\": tokenizer.eos_token_id\n}\n\n# -------------------------------\n# 3. Batching and parallel processing\n# -------------------------------\ndef process_batch(batch_images, batch_size=4):\n    \"\"\"Process images in batches for better GPU utilization\"\"\"\n    try:\n        with torch.inference_mode():\n            responses = []\n            for i in range(0, len(batch_images), batch_size):\n                batch = batch_images[i:i+batch_size]\n                batch_responses, _ = model.chat(\n                    tokenizer,\n                    images=batch,\n                    question=prompt,\n                    generation_config=generation_config,\n                    history=None,\n                    return_history=True\n                )\n                responses.extend(batch_responses)\n            return responses\n    except Exception as e:\n        logger.error(f\"Batch processing failed: {e}\")\n        return [\"\"] * len(batch_images)\n\n# -------------------------------\n# 4. Enhanced prompt and processing\n# -------------------------------\nprompt = \"\"\"<image>\\nExtract receipt data as JSON with these fields:\n- Company Name, Address, Contact Information (Telephone, Email)\n- Date (DD/MM/YYYY), Time (HH:MM:SS)\n- Product Details (Name, ID, Price, Quantity)\n- Total Amount, Amount Paid, Change Returned\nReturn valid JSON only, no markdown.\"\"\"\n\ndef process_folder(input_folder, output_folder, batch_size=4):\n    image_files = sorted(glob.glob(os.path.join(input_folder, \"*.png\")))\n    logger.info(f\"Processing {len(image_files)} images in {input_folder}\")\n    \n    # Process in batches\n    for batch_idx in tqdm(range(0, len(image_files), batch_size), \n                        desc=\"Processing batches\"):\n        batch = image_files[batch_idx:batch_idx+batch_size]\n        \n        # Process batch\n        responses = process_batch(batch, batch_size)\n        \n        # Save results\n        for img_path, response in zip(batch, responses):\n            try:\n                base_name = os.path.splitext(os.path.basename(img_path))[0]\n                output_file = os.path.join(output_folder, f\"{base_name}.json\")\n                \n                # Clean response\n                response = response.strip()\n                if '```json' in response:\n                    response = response.split('```json')[1].split('```')[0].strip()\n                \n                # Save JSON\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(json.loads(response), f, indent=4)\n                    \n            except Exception as e:\n                logger.error(f\"Failed to save {img_path}: {e}\")\n\n# -------------------------------\n# 5. Main execution with parallel folder processing\n# -------------------------------\ndef main():\n    dataset_base = \"/kaggle/input/find-it-again-dataset/findit2\"\n    output_base = \"/kaggle/working/extracted_jsons\"\n    \n    with ThreadPoolExecutor(max_workers=3) as executor:\n        futures = []\n        for split in [\"train\", \"val\", \"test\"]:\n            input_folder = os.path.join(dataset_base, split)\n            output_folder = os.path.join(output_base, split)\n            \n            if os.path.isdir(input_folder):\n                os.makedirs(output_folder, exist_ok=True)\n                futures.append(\n                    executor.submit(process_folder, input_folder, output_folder)\n                )\n            else:\n                logger.error(f\"Missing folder: {input_folder}\")\n                \n        for future in futures:\n            future.result()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport json\nimport torch\nimport logging\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm import tqdm  # for progress bar\n\n# Disable FLASH_ATTN if needed\nos.environ[\"FLASH_ATTN_DISABLE\"] = \"1\"\n\n# -------------------------------\n# 1. Set up logging configuration\n# -------------------------------\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\n\n# -------------------------------\n# 2. Set up the model and tokenizer for GPU\n# -------------------------------\nmodel_path = 'h2oai/h2ovl-mississippi-2b'\nmodel = AutoModel.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).eval().cuda()  # Running on GPU\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n\n# -------------------------------\n# 3. Define the prompt template for structured extraction\n# -------------------------------\nprompt = (\"\"\"<image>\\n\nPlease extract the following information in JSON format:\n- Company Name\n- Address\n- Contact Information (Telephone, Email)\n- Date (DD/MM/YYYY format)\n- Time (HH:MM:SS format)\n- Product Details (for each product: Name, ID, Price, Quantity)\n- Total Amount\n- Amount Paid\n- Change Returned \nDo not include json tags like ```json or ``` in the output. Just return the json object.\nAlways return the output as a valid JSON object with keys corresponding to these fields.\n\"\"\")\n\n# -------------------------------\n# 4. Function to process one folder of images in batches and save JSON output to a specified output folder\n# -------------------------------\ndef process_folder(input_folder, output_folder, batch_size=8):\n    image_files = glob.glob(os.path.join(input_folder, \"*.png\"))\n    logger.info(f\"Found {len(image_files)} images in {input_folder}.\")\n    \n    for i in tqdm(range(0, len(image_files), batch_size), desc=f\"Processing {os.path.basename(input_folder)}\"):\n        batch_files = image_files[i:i+batch_size]\n        logger.info(f\"Processing batch: {batch_files}\")\n        try:\n            # Pass the list of image paths to model.chat for batch processing\n            response, history = model.chat(\n                tokenizer, batch_files, prompt, generation_config, history=None, return_history=True\n            )\n        except Exception as e:\n            logger.error(f\"Error processing batch {batch_files}: {e}\")\n            continue\n        \n        # Ensure response is a list. If it's a string (single output), wrap it in a list.\n        if isinstance(response, str):\n            response = [response]\n        \n        # Check that the number of responses matches the batch length.\n        if len(response) != len(batch_files):\n            logger.warning(f\"Number of responses ({len(response)}) does not match batch length ({len(batch_files)}).\")\n        \n        for file, resp in zip(batch_files, response):\n            logger.info(f\"Raw response for {file}: {resp}\")\n            try:\n                r = resp.strip()\n                # Remove markdown if present\n                if '```json' in r:\n                    r = r.split('```json')[1].split('```')[0].strip()\n                extracted_data = json.loads(r)\n            except Exception as e:\n                logger.warning(f\"Error parsing JSON for {file}: {e}\")\n                extracted_data = {\"extracted_text\": r}\n            \n            base_name = os.path.splitext(os.path.basename(file))[0]\n            output_file = os.path.join(output_folder, f\"{base_name}.json\")\n            try:\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(extracted_data, f, indent=4)\n                logger.info(f\"Saved extracted data to {output_file}\")\n            except Exception as e:\n                logger.error(f\"Error saving file {output_file}: {e}\")\n\n# -------------------------------\n# 5. Process dataset folders: train, val, and test and save outputs accordingly\n# -------------------------------\ndataset_base = \"/kaggle/input/find-it-again-dataset/findit2\"  # Update with your dataset path\noutput_base = \"/kaggle/working/extracted_jsons\"  # Base output folder for JSON files\n\nfor split in [\"train\", \"val\", \"test\"]:\n    input_folder = os.path.join(dataset_base, split)\n    output_folder = os.path.join(output_base, split)\n    if os.path.isdir(input_folder):\n        os.makedirs(output_folder, exist_ok=True)\n        logger.info(f\"\\nProcessing folder: {input_folder}\")\n        process_folder(input_folder, output_folder, batch_size=4)\n    else:\n        logger.error(f\"Folder {input_folder} does not exist.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport json\nimport torch\nimport logging\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm import tqdm  # for progress bar\n\n# Disable FLASH_ATTN if needed\nos.environ[\"FLASH_ATTN_DISABLE\"] = \"1\"\n\n# -------------------------------\n# 1. Set up logging configuration\n# -------------------------------\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\n\n# -------------------------------\n# 2. Set up the model and tokenizer for GPU\n# -------------------------------\nmodel_path = 'h2oai/h2ovl-mississippi-2b'\nmodel = AutoModel.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).eval().cuda()  # Running on GPU; remove .cuda() if using CPU\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n\n# -------------------------------\n# 3. Define the prompt template for structured extraction\n# -------------------------------\nprompt = (\"\"\"<image>\\n\nPlease extract the following information in JSON format:\n- Company Name\n- Address\n- Contact Information (Telephone, Email)\n- Date (DD/MM/YYYY format)\n- Time (HH:MM:SS format)\n- Product Details (for each product: Name, ID, Price, Quantity)\n- Total Amount\n- Amount Paid\n- Change Returned \nDo not include json tags like ```json or ``` in the output. Just return the json object.\nAlways return the output as a valid JSON object with keys corresponding to these fields.\n\"\"\")\n\n# -------------------------------\n# 4. Function to process one folder of images in batches and save JSON output to a specified output folder\n# -------------------------------\ndef process_folder(input_folder, output_folder, batch_size=4):\n    image_files = glob.glob(os.path.join(input_folder, \"*.png\"))\n    logger.info(f\"Found {len(image_files)} images in {input_folder}.\")\n    \n    for i in tqdm(range(0, len(image_files), batch_size), desc=f\"Processing {os.path.basename(input_folder)}\"):\n        batch_files = image_files[i:i+batch_size]\n        logger.info(f\"Processing batch: {batch_files}\")\n        try:\n            # Pass the list of image paths for batch processing\n            response, history = model.chat(\n                tokenizer, batch_files, prompt, generation_config, history=None, return_history=True\n            )\n        except Exception as e:\n            logger.error(f\"Error processing batch {batch_files}: {e}\")\n            continue\n        \n        # Ensure response is a list; if not, wrap it\n        if isinstance(response, str):\n            response = [response]\n        \n        # Check number of responses matches number of files\n        if len(response) != len(batch_files):\n            logger.warning(f\"Batch size mismatch: {len(response)} responses for {len(batch_files)} files.\")\n        \n        for file, resp in zip(batch_files, response):\n            logger.info(f\"Raw response for {file}: {resp}\")\n            try:\n                r = resp.strip()\n                # If markdown markers are present, remove them.\n                if '```json' in r:\n                    r = r.split('```json')[1].split('```')[0].strip()\n                extracted_data = json.loads(r)\n            except Exception as e:\n                logger.warning(f\"Error parsing JSON for {file}: {e}\")\n                extracted_data = {\"extracted_text\": r}\n            \n            base_name = os.path.splitext(os.path.basename(file))[0]\n            output_file = os.path.join(output_folder, f\"{base_name}.json\")\n            try:\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(extracted_data, f, indent=4)\n                logger.info(f\"Saved extracted data to {output_file}\")\n            except Exception as e:\n                logger.error(f\"Error saving file {output_file}: {e}\")\n\n# -------------------------------\n# 5. Process dataset folders: train, val, and test and save outputs accordingly\n# -------------------------------\ndataset_base = \"/kaggle/input/find-it-again-dataset/findit2\"  # Update with your dataset path\noutput_base = \"/kaggle/working/jsons_files\"  # Local output folder on Kaggle Notebook\n\nfor split in [\"train\", \"val\", \"test\"]:\n    input_folder = os.path.join(dataset_base, split)\n    output_folder = os.path.join(output_base, split)\n    if os.path.isdir(input_folder):\n        os.makedirs(output_folder, exist_ok=True)\n        logger.info(f\"\\nProcessing folder: {input_folder}\")\n        process_folder(input_folder, output_folder, batch_size=4)\n    else:\n        logger.error(f\"Folder {input_folder} does not exist.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:45:33.037358Z","iopub.execute_input":"2025-03-28T11:45:33.037934Z","iopub.status.idle":"2025-03-28T12:01:48.431095Z","shell.execute_reply.started":"2025-03-28T11:45:33.037901Z","shell.execute_reply":"2025-03-28T12:01:48.430189Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3855260e5b5549a284110eb186b629a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_h2ovl_chat.py:   0%|          | 0.00/3.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4639340c786c48ab8480c2776137ff76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_intern_vit.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded4309be4a142a29e7d565c6d311fce"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- configuration_h2ovl_chat.py\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_h2ovl_chat.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fecbf291d7534f60a3d18d0db22658e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversation.py:   0%|          | 0.00/5.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbfb95f9a1b34d56a8772a558ceba6f4"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_intern_vit.py:   0%|          | 0.00/18.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75f0f7bf07274ee889c2e9863363e327"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- modeling_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"image_process.py:   0%|          | 0.00/6.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc9db1553314b919cfd43969fee9196"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- image_process.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/h2oai/h2ovl-mississippi-2b:\n- modeling_h2ovl_chat.py\n- conversation.py\n- modeling_intern_vit.py\n- image_process.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"name":"stdout","text":"FlashAttention is not installed.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5435b9fd3ebe43fe8d0ad30c6df7d0f2"}},"metadata":{}},{"name":"stdout","text":"Warning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\nWarning: Flash Attention is not available, use_flash_attn is set to False.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/161 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19af4773a224c008ba5344558a4c465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3bdc3855df849b397f983926014a6f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d493764708e3489ca17e4a226b433f9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/199 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c5a05e9e4134f8295838bef7b9bda50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"961617f4f8bd413e8f223027af823aa1"}},"metadata":{}},{"name":"stderr","text":"Processing train: 100%|██████████| 145/145 [09:27<00:00,  3.91s/it]\nProcessing val: 100%|██████████| 48/48 [02:45<00:00,  3.45s/it]\nProcessing test: 100%|██████████| 55/55 [03:19<00:00,  3.63s/it]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}