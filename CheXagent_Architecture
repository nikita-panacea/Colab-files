{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11795580,"sourceType":"datasetVersion","datasetId":7407018}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade ipywidgets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.40.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForCausalLM, AutoConfig\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16 if device==\"cuda\" else torch.float32\n\n# Load config & processor\nconfig    = AutoConfig.from_pretrained(\"StanfordAIMI/CheXagent-2-3b\", trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-2-3b\", trust_remote_code=True)\n# Load full model\nmodel     = AutoModelForCausalLM.from_pretrained(\n    \"StanfordAIMI/CheXagent-2-3b\",\n    config=config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n).to(device)\n\n# Sub-modules\nprint(model)                           # full vision-language model\nprint(model.vision_model)              # the vision encoder (a CLIPVision/QFormers stack)\nprint(model.qformer)                   # cross-modal Q-former\nprint(model.language_model)            # decoder LLM :contentReference[oaicite:0]{index=0}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vision encoder submodule\nvision_encoder = model.vision_model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoConfig\n\nmodel_name = \"StanfordAIMI/CheXagent-8b\"\nconfig = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\nprint(config)\n\n# You can specifically look at the vision model's configuration if it's structured that way\nif hasattr(config, 'vision_config'):\n    print(\"\\nVision Encoder Configuration:\")\n    print(config.vision_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig\nimport torch\n\ndevice = \"cuda\"\ndtype = torch.float16\n\n# 1. Load processor + model (this will pull in the custom CheXagent code)\nprocessor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", trust_remote_code=True)\ngen_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"StanfordAIMI/CheXagent-8b\",\n    torch_dtype=dtype,\n    trust_remote_code=True\n)\n\n# 2. Inspect its submodules to find the vision encoder\nprint([n for n, _ in model.named_modules() if \"vision\" in n.lower()])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# assuming the encoder is at model.vision_model\nvision_encoder = model.vision_model\n# freeze if you like\nfor p in vision_encoder.parameters():\n    p.requires_grad = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\nimport torch\n\n# 1. load processor & full agent\nprocessor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", trust_remote_code=True)\ngeneration_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\nagent = AutoModelForCausalLM.from_pretrained(\n    \"StanfordAIMI/CheXagent-8b\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n)\n\n# 2. inspect vision sub-module\nvision_encoder = agent.vision_model\nprint(vision_encoder)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}