{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11795580,"sourceType":"datasetVersion","datasetId":7407018},{"sourceId":11807802,"sourceType":"datasetVersion","datasetId":7415702},{"sourceId":11808692,"sourceType":"datasetVersion","datasetId":7416346}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade ipywidgets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.40.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:36:23.955925Z","iopub.execute_input":"2025-05-27T15:36:23.956120Z","iopub.status.idle":"2025-05-27T15:36:36.249616Z","shell.execute_reply.started":"2025-05-27T15:36:23.956103Z","shell.execute_reply":"2025-05-27T15:36:36.248597Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.40.0\n  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.40.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.1\n    Uninstalling tokenizers-0.21.1:\n      Successfully uninstalled tokenizers-0.21.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import logging\nlogging.set_verbosity_error()\n\nfrom transformers import AutoConfig, AutoProcessor, AutoModelForCausalLM\nimport torch\n\ndevice = \"cpu\"\nconfig    = AutoConfig.from_pretrained(\"StanfordAIMI/CheXagent-2-3b\", trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-2-3b\", trust_remote_code=True)\n\nfull = AutoModelForCausalLM.from_pretrained(\n    \"StanfordAIMI/CheXagent-2-3b\",\n    config=config,\n    torch_dtype=torch.float32,\n    trust_remote_code=True,\n    device_map=\"cpu\"\n)\nprint(full.model.visual)       # this is the CLIPModel wrapper\nprint(full.model.visual.model) # this is the SiglipVisionTransformer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:09:47.817025Z","iopub.execute_input":"2025-05-27T07:09:47.817228Z","iopub.status.idle":"2025-05-27T07:11:36.177108Z","shell.execute_reply.started":"2025-05-27T07:09:47.817206Z","shell.execute_reply":"2025-05-27T07:11:36.176470Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 07:09:55.376340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748329795.558013      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748329795.610956      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f960b5ff9e924b3880b39d4c84a4200b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_chexagent.py:   0%|          | 0.00/9.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccbedb0faa6942289b563a6eb90c611a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc371f89000e4de386659eff995b4dc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_chexagent.py:   0%|          | 0.00/26.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c197e2dcf4b40358c76478a35590e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68d9224bc6d40db897b254453dd7e1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9375183c1a644332b8e6836fbdcdb019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2480bac75474ef4b5aeb4587fced51d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/769 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa5cdba7f754aa799413d5fa92d7a1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_chexagent.py:   0%|          | 0.00/53.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"379f63ba7ad44154bd6c495daf5b0f69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_visual.py:   0%|          | 0.00/8.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca3fa473c10482baeab6f599c1393a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/75.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0927b492c9498f8a5a09045937e913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b96831f068d4a568511bf52ef3a36dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c858673493e4c14b9dc8e06c071db74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea49ed266a3341bbb44a7c058a905145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/2.60G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d483a777c52841399384745296786e9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ded932ad754798ad309b45bc95201c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec81051232b47f990e516984f5b9b2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9b568a16f64475a4c19b8d6a2663bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7861b5ea76fc4712859c856ce0693a5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1de8245b704444bb565b46dabd754f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02bc6f59f1541e287747b056f367392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d964d177d513420db39e23bc51126b32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6903ffda601467b87a503841b24dc8a"}},"metadata":{}},{"name":"stdout","text":"CLIPModel(\n  (model): SiglipVisionTransformer(\n    (embeddings): SiglipVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n      (position_embedding): Embedding(1024, 1024)\n    )\n    (encoder): SiglipEncoder(\n      (layers): ModuleList(\n        (0-23): 24 x SiglipEncoderLayer(\n          (self_attn): SiglipAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): SiglipMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n          (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n    (head): SiglipMultiheadAttentionPoolingHead(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n      )\n      (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n      (mlp): SiglipMLP(\n        (activation_fn): GELUActivation()\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n      )\n    )\n  )\n  (attn_pool): Sequential(\n    (0): Linear(in_features=1024, out_features=10240, bias=True)\n    (1): GELUActivation()\n    (2): Linear(in_features=10240, out_features=2560, bias=True)\n  )\n  (ln_post): LayerNorm((2560,), eps=1e-06, elementwise_affine=True)\n)\nSiglipVisionTransformer(\n  (embeddings): SiglipVisionEmbeddings(\n    (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n    (position_embedding): Embedding(1024, 1024)\n  )\n  (encoder): SiglipEncoder(\n    (layers): ModuleList(\n      (0-23): 24 x SiglipEncoderLayer(\n        (self_attn): SiglipAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (mlp): SiglipMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n      )\n    )\n  )\n  (post_layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n  (head): SiglipMultiheadAttentionPoolingHead(\n    (attention): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n    (mlp): SiglipMLP(\n      (activation_fn): GELUActivation()\n      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import logging\nlogging.set_verbosity_error()\n\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\n# 1. Load the shared config\nconfig = AutoConfig.from_pretrained(\n    \"StanfordAIMI/CheXagent-2-3b\",\n    trust_remote_code=True\n)\n\n# 2. Instantiate the full model on CPU\nfull = AutoModelForCausalLM.from_pretrained(\n    \"StanfordAIMI/CheXagent-2-3b\",\n    config=config,\n    torch_dtype=torch.float32,  # CPU float32 to minimize precision issues\n    trust_remote_code=True,\n    device_map=\"cpu\",           # EVERYTHING stays on CPU\n    low_cpu_mem_usage=True\n).eval()\n\n# 3. Pull out the vision encoder and move it to GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nvision_encoder = full.model.visual.model  # this is the SiglipVisionTransformer\nvision_encoder = vision_encoder.to(device)\n\n# 4. (Optional) Delete the full model to free up CPU RAM\ndel full\ntorch.cuda.empty_cache()\n# Now `vision_encoder` lives on your GPU and is ready for fine-tuning!\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:36:48.374535Z","iopub.execute_input":"2025-05-27T15:36:48.374782Z","iopub.status.idle":"2025-05-27T15:38:27.715298Z","shell.execute_reply.started":"2025-05-27T15:36:48.374755Z","shell.execute_reply":"2025-05-27T15:38:27.714316Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b2f8223d6b45c9878e806b8c6b81ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_chexagent.py:   0%|          | 0.00/9.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf82c286a6534610b571cf371487080b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_chexagent.py:   0%|          | 0.00/53.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8fc1beb535949879cd28640a41d28ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_chexagent.py:   0%|          | 0.00/26.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6be3228feb954c458e4ea81234e5bc4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_visual.py:   0%|          | 0.00/8.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d1370aae3b46fb8739a8413de17217"}},"metadata":{}},{"name":"stderr","text":"2025-05-27 15:37:05.078669: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748360225.257381      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748360225.311501      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/75.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a468da88ae5a47429e854b9f01dd907b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f6ba3037ff545ac8194bc5e31dc65e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae41f90679de43dd846c60b30fbfe5c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6fca9c79c84444a9747b20c270f744d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/2.60G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cad2458f982443a9f33b150d7e14756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad4ce177add43aaaf8c634fd4b6873c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b51a3cebdc104bda8ca1844ffe15fbaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d165549e8014635b3d049fbbedb6f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e954381b0d04a1b9a2602c6061d7ba9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/769 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29dc3df5c24d49d4b246674666ba7017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80bccbb60ced462cb5c61ff58baab840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98cacd94e8124aa495f6447a2f2c1f29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791ea45aa832470d9748d3f6d0ed7e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4cbd910c5245a1bc955188a1424ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d81e16e93c04aa1ba1c9b132c383a23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0943ac48f174437fa7c88b8392115818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"114b9193f2014ad98b4804812ed88556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c008af4c6929491ca3f1ff8bc4d4b209"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os, time, warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics        import roc_auc_score\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\n# ———————————————\n# 1) Data & Sampler\n# ———————————————\ndf = pd.read_csv(\"/kaggle/input/chest-x-ray-ground-truth-labels/test_df/test_df.csv\")\nlabel_cols = df.columns.tolist()[1:]\ndf[label_cols] = df[label_cols].astype(\"float32\")\ndf_train, df_val = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n\ncounts     = df_train[label_cols].sum()\nN          = len(df_train)\npos_weight = torch.tensor(((N - counts) / counts).values, dtype=torch.float)\n\npreprocess = T.Compose([\n    T.Resize((512, 512)),\n    T.ToTensor(),\n    T.Normalize([0.48145466, 0.4578275, 0.40821073],\n                [0.26862954, 0.26130258, 0.27577711]),\n])\n\n# preprocess = T.Compose([\n#     T.Resize((448, 448), interpolation=T.InterpolationMode.BICUBIC),  # match 8b\n#     T.ToTensor(),        # [0…1]\n#     T.Normalize(         # same as before\n#         mean=[0.48145466, 0.4578275, 0.40821073],\n#         std =[0.26862954, 0.26130258, 0.27577711],\n#     ),\n# ])\n\n\nclass CXRDataset(Dataset):\n    def __init__(self, df, img_dir, tf):\n        self.df = df.reset_index(drop=True)\n        self.dir = img_dir\n        self.tf  = tf\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(f\"{self.dir}/{row.ImageName}\").convert(\"RGB\")\n        pv  = self.tf(img)\n        lbl = torch.from_numpy(row[label_cols].to_numpy(dtype=np.float32))\n        return pv, lbl\n\nIMAGE_DIR = \"/kaggle/input/chest-x-ray-images/test_data\"\ntrain_ds = CXRDataset(df_train, IMAGE_DIR, preprocess)\nval_ds   = CXRDataset(df_val,   IMAGE_DIR, preprocess)\n\nweights = ((pos_weight * df_train[label_cols].values).sum(axis=1) + 1.0)\nsampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n\ndef collate_fn(batch):\n    pvs  = torch.stack([b[0] for b in batch])\n    lbls = torch.stack([b[1] for b in batch])\n    return pvs, lbls\n\n# ———————————————\n# 2) Model Preparation\n# ———————————————\nconfig = AutoConfig.from_pretrained(\"StanfordAIMI/CheXagent-2-3b\", trust_remote_code=True)\nfull   = AutoModelForCausalLM.from_pretrained(\n    \"StanfordAIMI/CheXagent-2-3b\",\n    config=config,\n    torch_dtype=torch.float32,\n    trust_remote_code=True,\n    device_map=\"cpu\",\n    low_cpu_mem_usage=True\n).eval()\n\nvision_encoder = full.model.visual.model\ndel full\ntorch.cuda.empty_cache()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nvision_encoder.to(device)  # keep FP32\n\nclass CXRMultiLabel(nn.Module):\n    def __init__(self, vision, num_labels):\n        super().__init__()\n        self.vision = vision\n        in_dim = vision.config.hidden_size\n        self.head = nn.Sequential(\n            nn.Linear(in_dim, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_labels)\n        )\n    def forward(self, pv, labels=None):\n        out    = self.vision(pixel_values=pv, return_dict=True)\n        cls    = out.last_hidden_state[:, 0]\n        logits = self.head(cls)\n        loss   = None\n        if labels is not None:\n            loss = nn.BCEWithLogitsLoss(\n                pos_weight=self.pos_weight.to(logits.device)\n            )(logits, labels)\n        return {\"loss\": loss, \"logits\": logits}\n\nmodel = CXRMultiLabel(vision_encoder, num_labels=len(label_cols))\nmodel.pos_weight = pos_weight\nmodel.to(device)\n\n# ———————————————\n# 3) Dataloaders\n# ———————————————\ntrain_loader = DataLoader(\n    train_ds, batch_size=2, sampler=sampler,\n    num_workers=2, pin_memory=True, collate_fn=collate_fn\n)\nval_loader   = DataLoader(\n    val_ds, batch_size=4, shuffle=False,\n    num_workers=2, pin_memory=True, collate_fn=collate_fn\n)\n\n# ———————————————\n# 4) Optimizer & AMP\n# ———————————————\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\ntotal_steps = len(train_loader) * 5\nscheduler   = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\nscaler      = torch.cuda.amp.GradScaler()\n\n# ———————————————\n# 5) Training Loop\n# ———————————————\nfor epoch in range(1, 6):\n    model.train()\n    running_loss = 0.0\n    t0 = time.time()\n    for step, (pv, lbl) in enumerate(train_loader, 1):\n        pv, lbl = pv.to(device, non_blocking=True), lbl.to(device, non_blocking=True)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            out  = model(pv, lbl)\n            loss = out[\"loss\"]\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        running_loss += loss.item()\n        if step % 50 == 0:\n            print(f\"Epoch {epoch} step {step}/{len(train_loader)} loss {running_loss/step:.4f}\")\n    print(f\"→ Epoch {epoch} finished in {time.time()-t0:.1f}s avg loss {running_loss/len(train_loader):.4f}\")\n\n    model.eval()\n    all_logits, all_labels = [], []\n    with torch.no_grad():\n        for pv, lbl in val_loader:\n            pv = pv.to(device)\n            with torch.cuda.amp.autocast():\n                logits = model(pv)[\"logits\"]\n            all_logits.append(logits.cpu())\n            all_labels.append(lbl)\n    all_logits = torch.cat(all_logits)\n    all_labels = torch.cat(all_labels)\n\n    print(\"Val AUROC:\")\n    for i, cname in enumerate(label_cols):\n        try:\n            auc = roc_auc_score(all_labels[:, i].numpy(), all_logits[:, i].numpy())\n        except ValueError:\n            auc = float(\"nan\")\n        print(f\"  {cname:>20}: {auc:.3f}\")\n    print(\"-\"*40)\n\n\n# assuming `model` is your CXRMultiLabel\nmodel_save_path = \"cxr_multilabel_checkpoint\"\nos.makedirs(model_save_path, exist_ok=True)\n\n# 1) Save the vision backbone\nvision_encoder.save_pretrained(f\"{model_save_path}/vision_encoder\")\n\n# 2) Save your head & pos_weight\ntorch.save({\n    \"head_state\":   model.head.state_dict(),\n    \"pos_weight\":   model.pos_weight.cpu(),\n    \"num_labels\":   len(label_cols),\n}, f\"{model_save_path}/classification_head.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:47:12.027512Z","iopub.execute_input":"2025-05-27T15:47:12.027865Z","execution_failed":"2025-05-27T15:48:33.633Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0a68183746845c7bc16dc8a17608d91"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 step 50/3117 loss 1.5552\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}